<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on LiuHe&#39;s blog</title>
    <link>https://okery.github.io/post/</link>
    <description>Recent content in Posts on LiuHe&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2019 19:43:58 +0800</lastBuildDate>
    
	<atom:link href="https://okery.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>机器学习笔记-基本概念（三）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Thu, 19 Sep 2019 19:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid>
      <description>机器学习
线性回归分析 回归 ​ 回归是处理两个或两个以上变量之间互相依赖的定量关系的一种统计方法和技术，变量之间的关系并非确定的函数关系，通过一定的概率分布来描述
线性 ​ 线性的严格定义是一种映射关系，其映射关系满足可加性和其次性。通俗理解就是两个变量之间存在一次方函数关系，在平面坐标系中表现为一条直线。不满足线性即为非线性。
线性回归 ​ 线性回归：在回归分析中，如果自变量和因变量之间存在着线性关系，则被称为线性回归。如果只有一个因变量一个自变量，则称为一元线性回归，如果有一个因变量多个自变量，则被称作多元回归。
回归模型 ​ 回归模型的一般形式是：
​ $$y=f(x{1},x{2},x{3},&amp;hellip;.,x{p})+\varepsilon$$
​ f()函数为确定性关系，后者为随机误差
建立回归模型 流程 ​ 1.需求分析明确变量：了解相关需求，明确场景，清楚需要解释的指标（因变量），并根据相关业务知识选取与之有关的变量作为解释变量（自变量）
​ 2.数据收集加工：根据上一步分析得出的解释变量，去收集相关的数据（时序数据、截面数据等），对得到的数据进行清洗、加工，并根据数据情况调整解释变量，并判断是否满足基本假设。
​ 3.确定回归模型： 了解数据集，使用绘图工具绘制变量样本散点图或使用其他分析工具分析变量间的关系，根据结果选择回归模型，如：线性回归模型、指数形式的回归模型等。
​ 4.模型参数估计：模型确定后，基于收集、整理的样本数据，估计模型中的相关参数。最常用的方法是最小二乘法，在不满足基本假设的情况下还会采取岭回归、主成分回归、偏最小二乘法等
​ 最小二乘法：也叫做最小平方法，通过最小化误差的平方和寻找数据的最佳函数匹配的方法
​ 5.模型检验优化：参数确定后，得到模型。此时需要对模型进行统计意义上的检验，包括对回归方程的显著性检验、回归系数的显著性检验、拟合优度检验、异方差检验、多重共线性检验等。还要结合实际场景，判断该模型是否具有意义。
​ 6.模型部署应用
回归模型的特点 ​ 模型简单，建模和应用都比较容易
​ 由坚实的统计理论支撑
​ 定量分析各变量之间的关系
​ 模型预测结果可以通过误差分析精确了解
​ 但：
​ 假设条件比较多且相对严格
​ 变量选择对模型影响较大</description>
    </item>
    
    <item>
      <title>筛选质数算法</title>
      <link>https://okery.github.io/post/%E7%AD%9B%E9%80%89%E8%B4%A8%E6%95%B0%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 18 Sep 2019 20:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AD%9B%E9%80%89%E8%B4%A8%E6%95%B0%E7%AE%97%E6%B3%95/</guid>
      <description>筛选算法——筛选指定范围内的质数
案例 指定范围为：1-20
数组为：
​ 1 2 3 4 5
​ 6 7 8 9 10
​ 11 12 13 14 15
​ 16 17 18 19 20
​ 21 22 23 24 25
1.去掉2的倍数
2.去掉3的倍数
3.去掉4的倍数
4.去掉5的倍数
去掉方法为：
​ j = i * i
​ j = j + i
​ i二次幂肯定为i的倍数，所以不为质数，去掉
​ 从i * i 处开始，以i为步长往后对应的数肯定为i的倍数，所以不为质数，去掉
​ 本案例范围为1-25，所以最高去掉5的倍数即可
算法实现 # 筛选算法 # 筛选给定范围内的质数 # 给定范围，将数据存放进数组，将非质数所在位置设为0，返回数组 def find_prime_number(a): numbers = [] # 初始化数组，将数据填充进数组 for i in range(1, a+1): numbers.</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（一）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%80/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%80/</guid>
      <description>机器学习第零章  
机器学习流程  输入数据 特征工程 模型训练 模型部署 模型应用  相关基本概念 输入空间 ​ 将输入的可能取值的集合称作输入控件
输出空间 ​ 将输出的所有可能取值的集合称作输出空间
特征空间 ​ 特征：即属性。每个输入实例的各个组成部分称作原始特征，基于原始特征还可以扩展出更多的衍生特征
​ 特征向量：由多个特征组成的集合，称作特征向量
​ 特征空间：将特征向量存在的空间称作特征空间
假设空间 ​ 假设空间：由输入空间到输出空间的映射的集合，称作假设空间
机器学习方法三要素 方法 = 模型 + 策略 + 算法 模型： ​ 输入空间到输出空间的映射关系
策略： ​ 从假设空间众多的假设中选择到最有的模型的学习标准或规则
算法： ​ 学习模型的具体的计算方法，通常是求解最优化问题
模型分类 ​ 预测分类：分类 Classification
​ 预测取值：回归 Regression
​ 发现结构：聚类 Clustering
​ 发现异常数据：异常检测 Anomaly Detection
损失函数 ​ 用来衡量预测结果和真实结果之间的差距，其值越小，代表预测结果和真实结果越一致
​ 平方损失函数：预测结果与真实结果差的平方
​ $$ L(Y,f(x)) = (Y - f(x))^{2} $$</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（二）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BA%8C/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BA%8C/</guid>
      <description>机器学习笔记 
模型选择的原则 误差 ​ 是模型的预测输出值与其真实值之间的差异
训练 ​ 通过已知的样本数据进行学习，从而得到模型的过程
训练误差 ​ 模型作用与训练集时的误差
泛化 ​ 有具体的、个别的扩大为一般的，即从特殊到一半，成为泛化。对机器学习的模型来讲，泛化是指模型作用于新的样本数据（非训练集）
泛化误差 ​ 模型作用于新的样本数据时的误差
模型容量 ​ 是指拟合各种模型的能力
过拟合和欠拟合 ​ 是某个模型在训练集上表现很好，但是在新样本上表现差。模型将训练集的特征学习的太好，导致一些非普遍规律被模型接纳和体现，从而在训练集上表现好，但是对于新样本表现差。反之则成为欠拟合，即模型对训练集的一边性质学习较差，模型作用于训练集时表现不好
模型选择 ​ 针对某个具体的任务，通常会有多种模型可供选择，对同一个模型也会有多组参数，可以通过分析、评估模型的泛化误差，选择泛化误差最小的模型
模型的评估方法 评估思路 ​ 通过实验测试，对模型的泛化误差进行评估，选出泛化误差最小的模型。待测数据集全集未知，使用测试集进项泛化测试，测试误差即为泛化误差的近似
​ 测试集和训练集尽可能互斥
​ 测试集和训练集独立同分布
留出法 ​ 将已知数据集的划分极可能保持数据分布一致性，避免因数据划分过程引入为的偏差
​ 数据分割存在多种形式会导致不同的训练集、测试集划分，单次留出法结果往往存在偶然性，其稳定性较差，通常会进行若干次随机划分、重复实验评估取平均值作为评估结果
​ 数据集拆成两部分，每部分的规模设置会影响评估结果，测试、训练的比例通常为7：3
​ 测试集和训练集分开，缓解了过拟合，但一次划分，评估结果偶然性大，而且数据被拆分后，用户训练、测试的数据更小了
交叉验证法 ​ 将数据集划分k个大小相似的互斥的数据子集，子集数据尽可能保证数据分布的一致性（分层采样），每次从中选取一个数据集作为测试集，其余用作训练集，可以进行k次训练和测试，得到评估均值。该验证方法也称作k折交叉验证。使用不同的划分，重复p次，称作p次k折交叉验证
留一法 ​ 是k折交叉验证的特殊形式，将数据集分成两个，其中一个数据集记录条数为1，作为测试集使用，其余记录作为训练集模型。训练出的模型使用全部诗句集训练得到的模型接近，其评估结果比较准确。缺点是当数据集较大时，训练次数和规模较大。
​ 充分利用了所有样本，多次划分，评估结果相对稳定，但是计算比较繁琐，需要进行k次训练和评估
自助法 ​ 是一种产生样本的抽象方法，其实质是有放回的随机抽样。级从已知数据集中随机抽取一条数据，然后将该记录放入测试集同时放回原数据集，继续下一次冲一样，知道测试集中的数据条数满足要求。
​ 对总体的理论分布没有要求，但是无放回抽样引入了额外的偏差
几种方法的选择 ​ 已知数据集数量充足时，通常采用留出法或者k折交叉验证法
​ 对于已知数据集较小且难以有效划分训练集/测试集的时候，采用自助法
​ 对于已知数据集较小且可以有效划分训练集/测试集的时候，采用留一法
假设检验步骤 1.建立假设 ​ 根据具体的问题，建立假设</description>
    </item>
    
    <item>
      <title>欧几里得算法实现</title>
      <link>https://okery.github.io/post/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/</guid>
      <description> 欧几里得算法 欧几里得算法用来计算两个整数的最大公约数
公式 ​ gcd(a, b) = gcd(b, a mod b)
原理 ​ 两个整数的最大公约数等于其中较小的那个数和两数相除的最大公约数
实现 ​ 递归实现
def gcd_recursion(a, b): if b == 0: return a return gcd_re(b, a % b) print(gcd_re(6, 4))  ​ 非递归实现
def gcd_no_recursion(a, b): while b != 0: tmp = a % b a = b b = tmp return a  </description>
    </item>
    
    <item>
      <title>吴恩达机器学习——单变量线性回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>吴恩达机器学习——单变量线性回归
基本概念 ### 假设函数  ​ 我们得到一个训练集，并用其进行某种预测时，我们可以根据训练集得出一个假设函数。
​ 如下图所示，下图为某种行业效益值y与程视人口数量x的训练集
​ 根据图像特征，我们可以得出这些数据可以用下图函数来代表：
​ ​ 因此我们可以得出一个假设函数：
​
$$ h{\theta}=\theta{0}+\theta_{1}X $$ ​
代价函数 ​ 在作出假设函数之后，我们要对建设函数中的两个未知量$$\theta{0}$$和$$\theta{1}$$做出选择，即选择不同的$$\theta{0}$$$$\theta{1}$$，验证所选择的值是否能时假设函数最大程度的能跟数值相符合
​ 对于如何判断所选择的$$\theta{0}$$和$$\theta{1}$$时最好的，我们选择均方误差来作为衡量标准，即： $$ J(\theta)=\tfrac{1}{2m}\sum{i=0}^{m}(h{\theta}(x^{i})-y^{i})^{2} $$ ​ 此即为我们的代价函数，函数值绝对值越小表示$$\theta{0}$$和$$\theta{1}$$越符合目标值
梯度下降算法 前言 ​ 在求代价函数的最小值时，以二维图像来为例表示函数值变化规律来讲，假设函数值变化规律如下图所示
​ 1处为最小值点，我们发现如果根据函数特征看，当$$\Delta x$$与$$y&amp;rsquo;$$符号相反时，沿着这个方向可以从任何点到达最小值处，因此在学习过程中可以根据导数（当为多元函数时为梯度）来判断如何改变变量值，来是的求解过程更加快速。
梯度定义 ​ 梯度表示某一函数在该处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大
多元函数梯度表达式 $$ \bigtriangledown f = (\frac{\partial f}{\partial x{i}}) 其中(\bigtriangledown f){i}=\frac{\partial f}{\partial x_{i}} $$
根据梯度下降法则选择$$ \theta$$ ​ 在选取变量$$\theta$$值时，根据梯度下降法则应该按照以下规则选取$$\theta$$的值
​
$$ \theta = \theta - \alpha \bigtriangleup f $$ ​ 其中$$\alpha$$为学习率，自己选取,因为梯度的方向是函数f增长最快的方向，梯度的反方向是f降低最快的方向，所以选取梯度的负数方向。
梯度下降算法求解过程 ​ 1.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习——监督学习和无监督学习</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</guid>
      <description>吴恩达机器学习——监督学习和无监督学习 监督学习 特征：  有标签数据 2.直接反馈 3.预测结果/未来  对每个数据都有准确的反馈
两个子类 1.回归：Regression 预测结果为连续值即为回归
2.分类：Classification 预测结果为离散值即为分类
无监督学习 特征 1.无标签/目标 2.无反馈 3.寻找数据中移仓的结构
两个子类 1.聚类： 见数据按相似度聚类成不同分组
2.降维： 在保留数据结构和有用性的同时对数据进行压缩</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(matlab)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</guid>
      <description>吴恩达机器学习——单变量线性回归作业（matlab） 1.Simple Octave/MATLAB function warmUpExercise.m：生成一个5 * 5 的单位矩阵 实现代码：
A = eye(5)	 笔记:matlab中eye(n)函数生成一个n*n的单位矩阵
结果：
2.Plotting the Data plotData.m：绘制数据，将数据集绘制在二维空间 实现代码:
plot(x,y,&#39;rx&#39;,&#39;MarkerSize&#39;,10); %Plot the data ylabel(&#39;Profit in $10,000s&#39;);	%set the y-axis label xlabel(&#39;Pupulation of City in 10,000s&#39;); %set the x-axis label  笔记：plot(x,y,&amp;lsquo;rx&amp;rsquo;,&amp;lsquo;MarkerSize&amp;rsquo;,10)参数含义:x、y为点的坐标，rx表示用红色的X形状来表示点，MarkerSize、10表示X形状的大小
结果：
​ ​
3.Computing the cost J(theta) computeCost.m：计算代价函数值 假设函数： $$ h{\theta}(x)=\theta^{\tau}x=\theta{0}+\theta{1}x $$ 代价函数： $$ J(\theta)=\tfrac{1}{2m}\sum{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^{2} $$ 实现代码：
J = sum(((X * theta) - y) .^ 2)/(2*m);  笔记：.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(python)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</guid>
      <description>机器学习—单变量线性回归作业(python)
python代码实现链接： 代码</description>
    </item>
    
    <item>
      <title>机器学习笔记-最大似然估计</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid>
      <description>最大似然估计
原理 ​ 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果退出参数的大概值。思想是：已知某个参数从能使这个样本出现的概率最大，那么就干脆将这个参数作为估计的真实值</description>
    </item>
    
  </channel>
</rss>