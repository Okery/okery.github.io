<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on LiuHe&#39;s blog</title>
    <link>https://okery.github.io/post/</link>
    <description>Recent content in Posts on LiuHe&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2019 14:43:58 +0800</lastBuildDate>
    
	<atom:link href="https://okery.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>吴恩达机器学习——单变量线性回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>吴恩达机器学习——单变量线性回归
基本概念 ### 假设函数  ​ 我们得到一个训练集，并用其进行某种预测时，我们可以根据训练集得出一个假设函数。
​ 如下图所示，下图为某种行业效益值y与程视人口数量x的训练集
​ 根据图像特征，我们可以得出这些数据可以用下图函数来代表：
​ ​ 因此我们可以得出一个假设函数：
​
$$ h{\theta}=\theta{0}+\theta_{1}X $$ ​
代价函数 ​ 在作出假设函数之后，我们要对建设函数中的两个未知量$$\theta{0}$$和$$\theta{1}$$做出选择，即选择不同的$$\theta{0}$$$$\theta{1}$$，验证所选择的值是否能时假设函数最大程度的能跟数值相符合
​ 对于如何判断所选择的$$\theta{0}$$和$$\theta{1}$$时最好的，我们选择均方误差来作为衡量标准，即： $$ J(\theta)=\tfrac{1}{2m}\sum{i=0}^{m}(h{\theta}(x^{i})-y^{i})^{2} $$ ​ 此即为我们的代价函数，函数值绝对值越小表示$$\theta{0}$$和$$\theta{1}$$越符合目标值
梯度下降算法 前言 ​ 在求代价函数的最小值时，以二维图像来为例表示函数值变化规律来讲，假设函数值变化规律如下图所示
​ 1处为最小值点，我们发现如果根据函数特征看，当$$\Delta x$$与$$y&amp;rsquo;$$符号相反时，沿着这个方向可以从任何点到达最小值处，因此在学习过程中可以根据导数（当为多元函数时为梯度）来判断如何改变变量值，来是的求解过程更加快速。
梯度定义 ​ 梯度表示某一函数在该处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大
多元函数梯度表达式 $$ \bigtriangledown f = (\frac{\partial f}{\partial x{i}}) 其中(\bigtriangledown f){i}=\frac{\partial f}{\partial x_{i}} $$
根据梯度下降法则选择$$ \theta$$ ​ 在选取变量$$\theta$$值时，根据梯度下降法则应该按照以下规则选取$$\theta$$的值
​
$$ \theta = \theta - \alpha \bigtriangleup f $$ ​ 其中$$\alpha$$为学习率，自己选取,因为梯度的方向是函数f增长最快的方向，梯度的反方向是f降低最快的方向，所以选取梯度的负数方向。
梯度下降算法求解过程 ​ 1.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习——监督学习和无监督学习</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</guid>
      <description>吴恩达机器学习——监督学习和无监督学习 监督学习 特征：  有标签数据 2.直接反馈 3.预测结果/未来  对每个数据都有准确的反馈
两个子类 1.回归：Regression 预测结果为连续值即为回归
2.分类：Classification 预测结果为离散值即为分类
无监督学习 特征 1.无标签/目标 2.无反馈 3.寻找数据中移仓的结构
两个子类 1.聚类： 见数据按相似度聚类成不同分组
2.降维： 在保留数据结构和有用性的同时对数据进行压缩</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(matlab)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</guid>
      <description>吴恩达机器学习——单变量线性回归作业（matlab） 1.Simple Octave/MATLAB function warmUpExercise.m：生成一个5 * 5 的单位矩阵 实现代码：
A = eye(5)	 笔记:matlab中eye(n)函数生成一个n*n的单位矩阵
结果：
2.Plotting the Data plotData.m：绘制数据，将数据集绘制在二维空间 实现代码:
plot(x,y,&#39;rx&#39;,&#39;MarkerSize&#39;,10); %Plot the data ylabel(&#39;Profit in $10,000s&#39;);	%set the y-axis label xlabel(&#39;Pupulation of City in 10,000s&#39;); %set the x-axis label  笔记：plot(x,y,&amp;lsquo;rx&amp;rsquo;,&amp;lsquo;MarkerSize&amp;rsquo;,10)参数含义:x、y为点的坐标，rx表示用红色的X形状来表示点，MarkerSize、10表示X形状的大小
结果：
​ ​
3.Computing the cost J(theta) computeCost.m：计算代价函数值 假设函数： $$ h{\theta}(x)=\theta^{\tau}x=\theta{0}+\theta{1}x $$ 代价函数： $$ J(\theta)=\tfrac{1}{2m}\sum{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^{2} $$ 实现代码：
J = sum(((X * theta) - y) .^ 2)/(2*m);  笔记：.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(python)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</guid>
      <description>机器学习—单变量线性回归作业(python)
python代码实现链接： 代码</description>
    </item>
    
    <item>
      <title></title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E9%9B%B6%E7%AB%A0/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E9%9B%B6%E7%AB%A0/</guid>
      <description>机器学习第零章### 机器学习流程1. 输入数据2. 特征工程3. 模型训练4. 模型部署5. 模型应用### 相关基本概念#### 输入空间​	将输入的可能取值的集合称作输入控件#### 输出空间​	将输出的所有可能取值的集合称作输出空间#### 特征空间​	特征：即属性。每个输入实例的各个组成部分称作原始特征，基于原始特征还可以扩展出更多的衍生特征​	特征向量：由多个特征组成的集合，称作特征向量​	特征空间：将特征向量存在的空间称作特征空间#### 假设空间​	假设空间：由输入空间到输出空间的映射的集合，称作假设空间### 机器学习方法三要素#### 方法 = 模型 + 策略 + 算法	####	模型：​	输入空间到输出空间的映射关系####	策略：​	从假设空间众多的假设中选择到最有的模型的学习标准或规则####	算法：​	学习模型的具体的计算方法，通常是求解最优化问题 #### 模型分类​	预测分类：分类 Classification​	预测取值：回归 Regression​	发现结构：聚类 Clustering​	发现异常数据：异常检测 Anomaly Detection</description>
    </item>
    
  </channel>
</rss>