<!DOCTYPE html>
<html lang="en-us">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		
		
		<meta name="generator" content="Hugo 0.57.2" />
		<title>机器学习笔记-决策树 &middot; LiuHe&#39;s blog</title>
		<link rel="shortcut icon" href="https://okery.github.io/images/favicon.ico">
		<link rel="stylesheet" href="https://okery.github.io/css/style.css">
		<link rel="stylesheet" href="https://okery.github.io/css/highlight.css">

		
		<link rel="stylesheet" href="https://okery.github.io/css/monosocialiconsfont.css">
		

		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://okery.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://okery.github.io/posts'>Archive</a>
	<a href='https://okery.github.io/tags'>Tags</a>
	<a href='https://okery.github.io/about'>About</a>

	

	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        机器学习笔记-决策树
                    </h1>
                    <h2 class="headline">
                    Oct 5, 2019 14:43
                    · 485 words
                    · 3 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://okery.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<h3 id="优缺点">优缺点</h3>

<p>​   优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据</p>

<p>​   缺点：可能产生过度匹配问题</p>

<p>​   适用数据类型：数值型和标称型</p>

<h3 id="创建分支伪代码createbranch">创建分支伪代码createBranch()</h3>

<p>​   检测数据集中每个子项是否属于同一分类</p>

<p>​       IF so return 类标签</p>

<p>​       ELSE</p>

<p>​           寻找划分数据集的最好特征</p>

<p>​           划分数据集</p>

<p>​           创建分支节点</p>

<p>​               for 每个划分的子集</p>

<p>​                   调用函数createBranch并增加返回结果到分支节点中</p>

<p>​           return 分支节点</p>

<h3 id="一般流程">一般流程</h3>

<p>收集数据：可以使用任何方法</p>

<p>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化</p>

<p>分析数据：可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期</p>

<p>训练算法：构造书的数据结构</p>

<p>测试算法：使用经验树计算错误率</p>

<p>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好的理解数据的内在含义</p>

<h3 id="信息增益">信息增益</h3>

<h4 id="信息的定义">信息的定义</h4>

<p>如果待分类的事务可能划分在多个分类之中，则符号$$x_{i}$$的信息的定义为</p>

<p>$$l(x<em>{i})=-log</em>{2}p(x_{i})$$</p>

<p>其中$$p(x_{i})$$是选择该分类的概率</p>

<h4 id="熵的定义">熵的定义</h4>

<p>熵为信息的期望值，在信息论和概率统计中，熵是表示随机变量不确定性的变量。设X是一个取有限个值的离散随机变量，其概率分布为</p>

<p>$$P(X=x<em>{i}) = p</em>{i}. i=1,2,&hellip;,n$$</p>

<p>则随机变量X的熵定义为：</p>

<p>​   $$H=-\sum<em>{i=1}^{n}p(x</em>{i})log<em>{2}p(x</em>{i})$$</p>

<p>熵越大，随机变量的不确定性就越大</p>

<h4 id="条件熵">条件熵</h4>

<p>条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望</p>

<p>$$H(Y|X)=\sum<em>{i=1}^{n}p</em>{i}H(Y|X=x_{i})$$</p>

<p>这里，pi=P(X=xi), i=1,2,&hellip;,n</p>

<h4 id="信息增益-1">信息增益</h4>

<p>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度</p>

<p>特征A对训练数据集D的信息增益g(D, A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即</p>

<p>g(D,A)=H(D)-H(D|A)</p>

<p>信息增益大的特征具有更强的分类能力，所以在构建决策树时首先选择信息增益最大的特征作为子节点</p>

<h3 id="决策树工作原理">决策树工作原理</h3>

<p>得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个几点上，我们再次划分数据&hellip;&hellip;</p>

<p>所以使用递归的原则来处理数据集</p>

<p>递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类</p>

<h3 id="决策树的生成">决策树的生成</h3>

<h4 id="id3算法">ID3算法</h4>

<p>输入：训练数据集D，特征集A，阈值δ</p>

<p>输出：决策树T</p>

<ol>
<li>若D中所有实例属于同一类C,则T为单节点树，并将类C作为该节点的类标记，返回T</li>
<li>若A为空集，则T为单节点树，并将D中实例数最大的类C作为该节点的类标记，返回T</li>
<li>否则，继续计算A中各特征对D的信息增益，选择信息增益最大的特征Ag</li>
<li>如果Ag的信息增益小于阈值δ，则置T为单节点树，并将D中实例数最大的类C作为该节点的类标记，返回T</li>
<li>否则，对Ag的每一个可能指，依Ag=α将D分割为若干非空子集Di，将Di中实例数最大的类作为标记，构建子节点，由节点机器子节点构成树T，返回T</li>
<li>对第i个子节点，以Di为训练集，以A-{Ag}为特征集，递归的调用步1-步5，得到子树Ti,返回Ti</li>
</ol>

<h3 id="决策树的剪枝">决策树的剪枝</h3>

<p>决策树生成算法递归的产生决策树，知道不能继续下去为止，这样产生的树往往能拟合训练数据，但对测试数据的分类却不会很准确，即会出现过拟合现象。所以要对决策树进行简化，这一过程叫做剪枝。</p>

<h4 id="剪枝原理">剪枝原理</h4>

<p>剪枝通过极小化决策树整体的损失函数或代价函数来实现。</p>

<p>设树T的叶节点个数为|T|，t是树T的叶节点，该叶节点由Nt个样本点，其中k类的样本点由Ntk个，k=1,2,&hellip;,K，Ht(T)为叶节点t熵的经验熵，α&gt;=0为参数，则决策树学习的损失函数可以定义为</p>

<p>$$C<em>{\alpha}(T)=\sum</em>{t=1}^{|T|}N<em>{t}H</em>{T}(T)+\alpha|T|$$</p>

<p>其中经验熵为</p>

<p>$$H<em>{t}(T)=-\sum</em>{k}\frac{N<em>{tk}}{N</em>{t}}log(\frac{N<em>{tk}}{N</em>{t}})$$</p>

<p>剪枝就是当α确定是，选择损失函数最小的模型，即损失函数最小的子树。当α确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂程度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好</p>

<p>决策树生成只考虑了通过提高信息增益对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型</p>

<h4 id="树的剪枝">树的剪枝</h4>

<p>输入：生成算法产生的整个树T，参数α</p>

<p>输出：修剪后的子树Tα</p>

<ol>
<li><p>计算每个几点的经验生</p></li>

<li><p>递归的从树的叶节点向上回缩</p></li>
</ol>

<p>设一组叶节点回缩到其父节点之前与之后的整体树分别为Tb和Ta,其对应的损失函数值分别时Cα(Tb)和Cα(Ta)，如果Cα(Tb)&gt;=Cα(Ta),则进行剪枝，即将父节点变为新的叶节点</p>

<ol>
<li>返回2，直至不能继续未知，得到损失函数最小的子树Tα</li>
</ol>

<h3 id="代码实现">代码实现</h3>

<pre><code class="language-python"># -*- coding: utf-8 -*-
&quot;&quot;&quot;
@File    :   decision_tree_action2.py    
@Contact :   13132515202@163.com

@Modify Time      @Author    @Version    @Description
------------      -------    --------    -----------
2019/12/2 19:14   LiuHe      1.0         决策树代码实现
&quot;&quot;&quot;
from math import log
import operator


def calc_shannon_ent(dataset):
    &quot;&quot;&quot;
    计算香农熵
    :param dataset: 数据集
    :return: 数据集香农熵
    &quot;&quot;&quot;
    num_samples = len(dataset)
    label_counts = {}

    for allFeatureVector in dataset:
        current_label = allFeatureVector[-1]
        if current_label not in label_counts.keys():
            label_counts[current_label] = 0
        label_counts[current_label] += 1
    entropy = 0.0

    for key in label_counts:
        property = float(label_counts[key]) / num_samples
        entropy -= property * log(property, 2)

    return entropy


def creat_data_set():
    &quot;&quot;&quot;
    生成数据
    :return: 训练集
    &quot;&quot;&quot;
    dataset = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 0, 'no']]
    labels = ['no surfacing', 'flippers']

    return dataset, labels


def get_sub_data_set(dataset, colIndex, value):
    &quot;&quot;&quot;
    取出在该特征取值下的子数据集
    :param dataset: 数据集
    :param colIndex: 特征索引
    :param value: 特征所有取值
    :return: 子数据集
    &quot;&quot;&quot;
    sub_data_set = []

    for row_vector in dataset:
        if row_vector[colIndex] == value:
            sub_row_vector = row_vector[:colIndex]
            sub_row_vector.extend(row_vector[colIndex+1:])
            sub_data_set.append(sub_row_vector)

    return sub_data_set


def best_feat_to_get_subdataset(dataset):
    &quot;&quot;&quot;
    选择最优的特征, 以便得到最优的子数据集
    :param dataset: 数据集
    :return: 最优特征在数据集中的列索引
    &quot;&quot;&quot;

    num_feature = len(dataset[0]) - 1
    base_entropy = calc_shannon_ent(dataset)
    best_info_gain = 0.0
    best_feature = -1

    for i in range(num_feature):
        feat_i_values = [example[i] for example in dataset]
        unique_values = set(feat_i_values)
        feat_i_entropy = 0.0
        for value in unique_values:
            sub_data_set = get_sub_data_set(dataset, i, value)
            prob_i = len(sub_data_set)/float(len(dataset))
            feat_i_entropy += prob_i * calc_shannon_ent(sub_data_set)
        info_gain_i = base_entropy - feat_i_entropy
        if info_gain_i &gt; best_info_gain:
            best_info_gain = info_gain_i
            best_feature = i

    return best_feature


def most_class(class_list):
    &quot;&quot;&quot;
    找出该数据集个数最多的类别
    :param class_list: 子数据集的类别标签列
    :return: 子数据集中个数最多的类别标签
    &quot;&quot;&quot;
    class_count = {}

    for class_i in class_list:
        if class_i not in class_count.keys():
            class_count[class_i] = 0
        class_count[class_i] += 1
    sorted_class_count = sorted(class_count.iteritems(),
                                key=operator.itemgetter(1), reverse=True)

    return sorted_class_count[0][0]


def creat_tree(dataset, labels):
    &quot;&quot;&quot;
    构造树
    :param dataset: 数据集
    :param labels: 标签集
    :return: 树
    &quot;&quot;&quot;
    class_list = [example[-1] for example in dataset]
    if class_list.count(class_list[0]) == len(class_list):
        return class_list[0]
    if len(dataset[0]) == 1:
        return most_class(class_list)

    best_feat = best_feat_to_get_subdataset(dataset)
    best_feat_label = labels[best_feat]
    my_tree = {best_feat_label: {}}

    del (labels[best_feat])
    best_feat_values = [example[best_feat] for example in dataset]
    unique_best_feat_values = set(best_feat_values)
    for value in unique_best_feat_values:
        sub_dataset = best_feat_to_get_subdataset(dataset, best_feat, value)
        sub_labels = labels[:]
        my_tree[best_feat_label][value] = creat_tree(sub_dataset, sub_labels)

    return my_tree


def classify(input_tree, feat_labels, test_feat_value):
    &quot;&quot;&quot;
    调用训练决策树对测试数据打上类别标签
    :param input_tree: 测试特征数据
    :param feat_labels: 测试特征数据
    :param test_feat_value: 测试特征数据
    :return: 测试特征数据所属类别
    &quot;&quot;&quot;
    first_str = input_tree.keys()[0]
    second_dict = input_tree[first_str]
    feat_index = feat_labels.index(first_str)
    for firstStr_value in second_dict.keys():
        if test_feat_value[feat_index] == firstStr_value:
            if type(second_dict[firstStr_value]).__name__ == 'dict':
                class_label = classify(second_dict[firstStr_value], feat_labels, test_feat_value)
            else:
                class_label = second_dict[firstStr_value]

    return class_label



</code></pre>

                </section>
            </article>

            

            

            

            <footer id="footer">
    
    <p class="small">
    
       © Copyright 2019 <i class="fa fa-heart" aria-hidden="true"></i> 
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="https://okery.github.io/js/jquery-3.3.1.min.js"></script>
<script src="https://okery.github.io/js/main.js"></script>
<script src="https://okery.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>







    </body>
</html>
