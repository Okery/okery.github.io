<!DOCTYPE html>
<html lang="en-us">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		
		
		
		<meta name="generator" content="Hugo 0.57.2" />
		<title>机器学习笔记-决策树 &middot; LiuHe&#39;s blog</title>
		<link rel="shortcut icon" href="https://okery.github.io/images/favicon.ico">
		<link rel="stylesheet" href="https://okery.github.io/css/style.css">
		<link rel="stylesheet" href="https://okery.github.io/css/highlight.css">

		
		<link rel="stylesheet" href="https://okery.github.io/css/monosocialiconsfont.css">
		

		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='https://okery.github.io/'> <span class="arrow">←</span>Home</a>
	
	<a href='https://okery.github.io/posts'>Archive</a>
	<a href='https://okery.github.io/tags'>Tags</a>
	<a href='https://okery.github.io/about'>About</a>

	

	
</nav>


        <section id="wrapper" class="post">
            <article>
                <header>
                    <h1>
                        机器学习笔记-决策树
                    </h1>
                    <h2 class="headline">
                    Oct 5, 2019 14:43
                    · 95 words
                    · 1 minute read
                      <span class="tags">
                      
                      
                          
                              <a href="https://okery.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<h3 id="优缺点">优缺点</h3>

<p>​   优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据</p>

<p>​   缺点：可能产生过度匹配问题</p>

<p>​   适用数据类型：数值型和标称型</p>

<h3 id="创建分支伪代码createbranch">创建分支伪代码createBranch()</h3>

<p>​   检测数据集中每个子项是否属于同一分类</p>

<p>​       IF so return 类标签</p>

<p>​       ELSE</p>

<p>​           寻找划分数据集的最好特征</p>

<p>​           划分数据集</p>

<p>​           创建分支节点</p>

<p>​               for 每个划分的子集</p>

<p>​                   调用函数createBranch并增加返回结果到分支节点中</p>

<p>​           return 分支节点</p>

<h3 id="一般流程">一般流程</h3>

<p>收集数据：可以使用任何方法</p>

<p>准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化</p>

<p>分析数据：可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期</p>

<p>训练算法：构造书的数据结构</p>

<p>测试算法：使用经验树计算错误率</p>

<p>使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好的理解数据的内在含义</p>

<h3 id="信息增益">信息增益</h3>

<h4 id="信息的定义">信息的定义</h4>

<p>如果待分类的事务可能划分在多个分类之中，则符号$$x_{i}$$的信息的定义为</p>

<p>$$l(x<em>{i})=-log</em>{2}p(x_{i})$$</p>

<p>其中$$p(x_{i})$$是选择该分类的概率</p>

<h4 id="熵的定义">熵的定义</h4>

<p>熵为信息的期望值，在信息论和概率统计中，熵是表示随机变量不确定性的变量。设X是一个取有限个值的离散随机变量，其概率分布为</p>

<p>$$P(X=x<em>{i}) = p</em>{i}. i=1,2,&hellip;,n$$</p>

<p>则随机变量X的熵定义为：</p>

<p>​   $$H=-\sum<em>{i=1}^{n}p(x</em>{i})log<em>{2}p(x</em>{i})$$</p>

<p>熵越大，随机变量的不确定性就越大</p>

<h4 id="条件熵">条件熵</h4>

<p>条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望</p>

<p>$$H(Y|X)=\sum<em>{i=1}^{n}p</em>{i}H(Y|X=x_{i})$$</p>

<p>这里，pi=P(X=xi), i=1,2,&hellip;,n</p>

<h4 id="信息增益-1">信息增益</h4>

<p>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度</p>

<p>特征A对训练数据集D的信息增益g(D, A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即</p>

<p>g(D,A)=H(D)-H(D|A)</p>

<p>信息增益大的特征具有更强的分类能力，所以在构建决策树时首先选择信息增益最大的特征作为子节点</p>

<h3 id="决策树工作原理">决策树工作原理</h3>

<p>得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个几点上，我们再次划分数据&hellip;&hellip;</p>

<p>所以使用递归的原则来处理数据集</p>

<p>递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类</p>

<h3 id="决策树的生成">决策树的生成</h3>

<h4 id="id3算法">ID3算法</h4>

<p>输入：训练数据集D，特征集A，阈值δ</p>

<p>输出：决策树T</p>

<ol>
<li>若D中所有实例属于同一类C,则T为单节点树，并将类C作为该节点的类标记，返回T</li>
<li>若A为空集，则T为单节点树，并将D中实例数最大的类C作为该节点的类标记，返回T</li>
<li>否则，继续计算A中各特征对D的信息增益，选择信息增益最大的特征Ag</li>
<li>如果Ag的信息增益小于阈值δ，则置T为单节点树，并将D中实例数最大的类C作为该节点的类标记，返回T</li>
<li>否则，对Ag的每一个可能指，依Ag=α将D分割为若干非空子集Di，将Di中实例数最大的类作为标记，构建子节点，由节点机器子节点构成树T，返回T</li>
<li>对第i个子节点，以Di为训练集，以A-{Ag}为特征集，递归的调用步1-步5，得到子树Ti,返回Ti</li>
</ol>

<h3 id="决策树的剪枝">决策树的剪枝</h3>

<p>决策树生成算法递归的产生决策树，知道不能继续下去为止，这样产生的树往往能拟合训练数据，但对测试数据的分类却不会很准确，即会出现过拟合现象。所以要对决策树进行简化，这一过程叫做剪枝。</p>

<h4 id="剪枝原理">剪枝原理</h4>

<p>剪枝通过极小化决策树整体的损失函数或代价函数来实现。</p>

<p>设树T的叶节点个数为|T|，t是树T的叶节点，该叶节点由Nt个样本点，其中k类的样本点由Ntk个，k=1,2,&hellip;,K，Ht(T)为叶节点t熵的经验熵，α&gt;=0为参数，则决策树学习的损失函数可以定义为</p>

<p>$$C<em>{\alpha}(T)=\sum</em>{t=1}^{|T|}N<em>{t}H</em>{T}(T)+\alpha|T|$$</p>

<p>其中经验熵为</p>

<p>$$H<em>{t}(T)=-\sum</em>{k}\frac{N<em>{tk}}{N</em>{t}}log(\frac{N<em>{tk}}{N</em>{t}})$$</p>

<p>剪枝就是当α确定是，选择损失函数最小的模型，即损失函数最小的子树。当α确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂程度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好</p>

<p>决策树生成只考虑了通过提高信息增益对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型</p>

<h4 id="树的剪枝">树的剪枝</h4>

<p>输入：生成算法产生的整个树T，参数α</p>

<p>输出：修剪后的子树Tα</p>

<ol>
<li><p>计算每个几点的经验生</p></li>

<li><p>递归的从树的叶节点向上回缩</p></li>
</ol>

<p>设一组叶节点回缩到其父节点之前与之后的整体树分别为Tb和Ta,其对应的损失函数值分别时Cα(Tb)和Cα(Ta)，如果Cα(Tb)&gt;=Cα(Ta),则进行剪枝，即将父节点变为新的叶节点</p>

<ol>
<li>返回2，直至不能继续未知，得到损失函数最小的子树Tα</li>
</ol>

                </section>
            </article>

            

            

            

            <footer id="footer">
    
    <p class="small">
    
       © Copyright 2019 <i class="fa fa-heart" aria-hidden="true"></i> 
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>

        </section>

        <script src="https://okery.github.io/js/jquery-3.3.1.min.js"></script>
<script src="https://okery.github.io/js/main.js"></script>
<script src="https://okery.github.io/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>







    </body>
</html>
