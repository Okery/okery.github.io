<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LiuHe&#39;s blog</title>
    <link>https://okery.github.io/</link>
    <description>Recent content on LiuHe&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2019 20:23:58 +0800</lastBuildDate>
    
	<atom:link href="https://okery.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>筛选质数算法</title>
      <link>https://okery.github.io/post/%E7%AD%9B%E9%80%89%E8%B4%A8%E6%95%B0%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 18 Sep 2019 20:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AD%9B%E9%80%89%E8%B4%A8%E6%95%B0%E7%AE%97%E6%B3%95/</guid>
      <description>筛选算法——筛选指定范围内的质数
案例 指定范围为：1-20
数组为：
​ 1 2 3 4 5
​ 6 7 8 9 10
​ 11 12 13 14 15
​ 16 17 18 19 20
​ 21 22 23 24 25
1.去掉2的倍数
2.去掉3的倍数
3.去掉4的倍数
4.去掉5的倍数
去掉方法为：
​ j = i * i
​ j = j + i
​ i二次幂肯定为i的倍数，所以不为质数，去掉
​ 从i * i 处开始，以i为步长往后对应的数肯定为i的倍数，所以不为质数，去掉
​ 本案例范围为1-25，所以最高去掉5的倍数即可
算法实现 # 筛选算法 # 筛选给定范围内的质数 # 给定范围，将数据存放进数组，将非质数所在位置设为0，返回数组 def find_prime_number(a): numbers = [] # 初始化数组，将数据填充进数组 for i in range(1, a+1): numbers.</description>
    </item>
    
    <item>
      <title>机器学习第一章笔记</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E7%AB%A0/</guid>
      <description>机器学习第一章 
模型选择的原则 误差 ​ 是模型的预测输出值与其真实值之间的差异
训练 ​ 通过已知的样本数据进行学习，从而得到模型的过程
训练误差 ​ 模型作用与训练集时的误差
泛化 ​ 有具体的、个别的扩大为一般的，即从特殊到一半，成为泛化。对机器学习的模型来讲，泛化是指模型作用于新的样本数据（非训练集）
泛化误差 ​ 模型作用于新的样本数据时的误差
模型容量 ​ 是指拟合各种模型的能力
过拟合和欠拟合 ​ 是某个模型在训练集上表现很好，但是在新样本上表现差。模型将训练集的特征学习的太好，导致一些非普遍规律被模型接纳和体现，从而在训练集上表现好，但是对于新样本表现差。反之则成为欠拟合，即模型对训练集的一边性质学习较差，模型作用于训练集时表现不好
模型选择 ​ 针对某个具体的任务，通常会有多种模型可供选择，对同一个模型也会有多组参数，可以通过分析、评估模型的泛化误差，选择泛化误差最小的模型
模型的评估方法 评估思路 ​ 通过实验测试，对模型的泛化误差进行评估，选出泛化误差最小的模型。待测数据集全集未知，使用测试集进项泛化测试，测试误差即为泛化误差的近似
​ 测试集和训练集尽可能互斥
​ 测试集和训练集独立同分布
留出法 ​ 将已知数据集的划分极可能保持数据分布一致性，避免因数据划分过程引入为的偏差
​ 数据分割存在多种形式会导致不同的训练集、测试集划分，单次留出法结果往往存在偶然性，其稳定性较差，通常会进行若干次随机划分、重复实验评估取平均值作为评估结果
​ 数据集拆成两部分，每部分的规模设置会影响评估结果，测试、训练的比例通常为7：3
交叉验证法 ​ 将数据集划分k个大小相似的互斥的数据子集，子集数据尽可能保证数据分布的一致性（分层采样），每次从中选取一个数据集作为测试集，其余用作训练集，可以进行k次训练和测试，得到评估均值。该验证方法也称作k折交叉验证。使用不同的划分，重复p次，称作p次k折交叉验证</description>
    </item>
    
    <item>
      <title>机器学习第零章笔记</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E9%9B%B6%E7%AB%A0/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E9%9B%B6%E7%AB%A0/</guid>
      <description>机器学习第零章  
机器学习流程  输入数据 特征工程 模型训练 模型部署 模型应用  相关基本概念 输入空间 ​ 将输入的可能取值的集合称作输入控件
输出空间 ​ 将输出的所有可能取值的集合称作输出空间
特征空间 ​ 特征：即属性。每个输入实例的各个组成部分称作原始特征，基于原始特征还可以扩展出更多的衍生特征
​ 特征向量：由多个特征组成的集合，称作特征向量
​ 特征空间：将特征向量存在的空间称作特征空间
假设空间 ​ 假设空间：由输入空间到输出空间的映射的集合，称作假设空间
机器学习方法三要素 方法 = 模型 + 策略 + 算法 模型： ​ 输入空间到输出空间的映射关系
策略： ​ 从假设空间众多的假设中选择到最有的模型的学习标准或规则
算法： ​ 学习模型的具体的计算方法，通常是求解最优化问题
模型分类 ​ 预测分类：分类 Classification
​ 预测取值：回归 Regression
​ 发现结构：聚类 Clustering
​ 发现异常数据：异常检测 Anomaly Detection
损失函数 ​ 用来衡量预测结果和真实结果之间的差距，其值越小，代表预测结果和真实结果越一致
​ 平方损失函数：预测结果与真实结果差的平方
​ $$ L(Y,f(x)) = (Y - f(x))^{2} $$</description>
    </item>
    
    <item>
      <title>欧几里得算法实现</title>
      <link>https://okery.github.io/post/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95/</guid>
      <description> 欧几里得算法 欧几里得算法用来计算两个整数的最大公约数
公式 ​ gcd(a, b) = gcd(b, a mod b)
原理 ​ 两个整数的最大公约数等于其中较小的那个数和两数相除的最大公约数
实现 ​ 递归实现
def gcd_recursion(a, b): if b == 0: return a return gcd_re(b, a % b) print(gcd_re(6, 4))  ​ 非递归实现
def gcd_no_recursion(a, b): while b != 0: tmp = a % b a = b b = tmp return a  </description>
    </item>
    
    <item>
      <title>吴恩达机器学习——单变量线性回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>吴恩达机器学习——单变量线性回归
基本概念 ### 假设函数  ​ 我们得到一个训练集，并用其进行某种预测时，我们可以根据训练集得出一个假设函数。
​ 如下图所示，下图为某种行业效益值y与程视人口数量x的训练集
​ 根据图像特征，我们可以得出这些数据可以用下图函数来代表：
​ ​ 因此我们可以得出一个假设函数：
​
$$ h{\theta}=\theta{0}+\theta_{1}X $$ ​
代价函数 ​ 在作出假设函数之后，我们要对建设函数中的两个未知量$$\theta{0}$$和$$\theta{1}$$做出选择，即选择不同的$$\theta{0}$$$$\theta{1}$$，验证所选择的值是否能时假设函数最大程度的能跟数值相符合
​ 对于如何判断所选择的$$\theta{0}$$和$$\theta{1}$$时最好的，我们选择均方误差来作为衡量标准，即： $$ J(\theta)=\tfrac{1}{2m}\sum{i=0}^{m}(h{\theta}(x^{i})-y^{i})^{2} $$ ​ 此即为我们的代价函数，函数值绝对值越小表示$$\theta{0}$$和$$\theta{1}$$越符合目标值
梯度下降算法 前言 ​ 在求代价函数的最小值时，以二维图像来为例表示函数值变化规律来讲，假设函数值变化规律如下图所示
​ 1处为最小值点，我们发现如果根据函数特征看，当$$\Delta x$$与$$y&amp;rsquo;$$符号相反时，沿着这个方向可以从任何点到达最小值处，因此在学习过程中可以根据导数（当为多元函数时为梯度）来判断如何改变变量值，来是的求解过程更加快速。
梯度定义 ​ 梯度表示某一函数在该处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大
多元函数梯度表达式 $$ \bigtriangledown f = (\frac{\partial f}{\partial x{i}}) 其中(\bigtriangledown f){i}=\frac{\partial f}{\partial x_{i}} $$
根据梯度下降法则选择$$ \theta$$ ​ 在选取变量$$\theta$$值时，根据梯度下降法则应该按照以下规则选取$$\theta$$的值
​
$$ \theta = \theta - \alpha \bigtriangleup f $$ ​ 其中$$\alpha$$为学习率，自己选取,因为梯度的方向是函数f增长最快的方向，梯度的反方向是f降低最快的方向，所以选取梯度的负数方向。
梯度下降算法求解过程 ​ 1.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习——监督学习和无监督学习</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</guid>
      <description>吴恩达机器学习——监督学习和无监督学习 监督学习 特征：  有标签数据 2.直接反馈 3.预测结果/未来  对每个数据都有准确的反馈
两个子类 1.回归：Regression 预测结果为连续值即为回归
2.分类：Classification 预测结果为离散值即为分类
无监督学习 特征 1.无标签/目标 2.无反馈 3.寻找数据中移仓的结构
两个子类 1.聚类： 见数据按相似度聚类成不同分组
2.降维： 在保留数据结构和有用性的同时对数据进行压缩</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(matlab)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</guid>
      <description>吴恩达机器学习——单变量线性回归作业（matlab） 1.Simple Octave/MATLAB function warmUpExercise.m：生成一个5 * 5 的单位矩阵 实现代码：
A = eye(5)	 笔记:matlab中eye(n)函数生成一个n*n的单位矩阵
结果：
2.Plotting the Data plotData.m：绘制数据，将数据集绘制在二维空间 实现代码:
plot(x,y,&#39;rx&#39;,&#39;MarkerSize&#39;,10); %Plot the data ylabel(&#39;Profit in $10,000s&#39;);	%set the y-axis label xlabel(&#39;Pupulation of City in 10,000s&#39;); %set the x-axis label  笔记：plot(x,y,&amp;lsquo;rx&amp;rsquo;,&amp;lsquo;MarkerSize&amp;rsquo;,10)参数含义:x、y为点的坐标，rx表示用红色的X形状来表示点，MarkerSize、10表示X形状的大小
结果：
​ ​
3.Computing the cost J(theta) computeCost.m：计算代价函数值 假设函数： $$ h{\theta}(x)=\theta^{\tau}x=\theta{0}+\theta{1}x $$ 代价函数： $$ J(\theta)=\tfrac{1}{2m}\sum{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^{2} $$ 实现代码：
J = sum(((X * theta) - y) .^ 2)/(2*m);  笔记：.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(python)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</guid>
      <description>机器学习—单变量线性回归作业(python)
python代码实现链接： 代码</description>
    </item>
    
  </channel>
</rss>