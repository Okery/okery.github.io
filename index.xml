<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LiuHe&#39;s blog</title>
    <link>https://okery.github.io/</link>
    <description>Recent content on LiuHe&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 27 Oct 2019 14:43:58 +0800</lastBuildDate>
    
	<atom:link href="https://okery.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>leetcode刷题-442-数组中重复的数据</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-442-%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Sun, 27 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-442-%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E6%8D%AE/</guid>
      <description> 问题描述 给定一个整数数组 a，其中1 ≤ a[i] ≤ n （n为数组长度）, 其中有些元素出现两次而其他元素出现一次。
找到所有出现两次的元素。
你可以不用到任何额外空间并在O(n)时间复杂度内解决这个问题吗？
示例 输入: [4,3,2,7,8,2,3,1] 输出: [2,3]  实现 实现1 先进行排序，然后使用双指针判断
边界情况要注意，输入为空数组、一个元素的数组情况
def find_duplicates(nums): &amp;quot;&amp;quot;&amp;quot; 排序 然后双指针验证 &amp;quot;&amp;quot;&amp;quot; duplicate_num = [] nums_len = len(nums) pre_pointer = 0 rear_pointer = 1 nums.sort() while(pre_pointer &amp;lt;= nums_len-2 and nums_len != 0 and nums_len != 1): if nums[pre_pointer] == nums[rear_pointer]: duplicate_num.append(nums[pre_pointer]) pre_pointer += 2 rear_pointer += 2 else: pre_pointer += 1 rear_pointer += 1 return duplicate_num test_nums = [1, 2, 3, 3, 4] find_duplicates(test_nums)  </description>
    </item>
    
    <item>
      <title>python常用转义字符</title>
      <link>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B8%B8%E7%94%A8%E8%BD%AC%E4%B9%89%E5%AD%97%E7%AC%A6/</link>
      <pubDate>Fri, 25 Oct 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B8%B8%E7%94%A8%E8%BD%AC%E4%B9%89%E5%AD%97%E7%AC%A6/</guid>
      <description>   转义字符 描述     \&amp;lsquo; 单引号   \&amp;ldquo; 双引号   \ 反斜杠   \b 退格   \000 空额   \n 换行   \v 纵向制表符   \t 横向制表符   \r 回车   \f 换行    </description>
    </item>
    
    <item>
      <title>leetcode刷题-121-买卖股票的最佳时机</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-121-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA/</link>
      <pubDate>Fri, 25 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-121-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA/</guid>
      <description>问题描述 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。
如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。
注意你不能在买入股票前卖出股票。
示例 示例 1:
输入: [7,1,5,3,6,4] 输出: 5 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 示例 2:
输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。
实现 1 暴力破解 将所有的利润可能算出，选择最大的利润返回
时间复杂度O(n^2)
def max_profit(nums): &amp;quot;&amp;quot;&amp;quot; 暴力枚举 没考虑输入数组为空 但是测试用例没有提出相关策略 最大测试用例运行十七秒 &amp;quot;&amp;quot;&amp;quot; nums_len = len(nums) print(nums_len) max_profit = 0 for i in range(nums_len-1): for j in range(i+1, nums_len): if nums[j] &amp;gt; nums[i]: current_profit = nums[j] - nums[i] if max_profit &amp;lt; current_profit: max_profit = current_profit return max_profit  2 遍历一次 最大利润一定在当前情况下的最小值(current_min_profit)与最大值(current_min_profit)的差值中，因此遍历数组，找出当前最小值(current_min_profit)与最大值(current_max_profit)，计算差值并与当前最大利润作比较，当前最小值(current_min_profit)大于当前值时，当前最大值与当前最大值同时更新成当前元素，开始向后遍历并进行上述操作</description>
    </item>
    
    <item>
      <title>leetcode刷题-136-只出现一次的数字</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-136-%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/</link>
      <pubDate>Fri, 25 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-136-%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/</guid>
      <description>问题描述 给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。
示例 示例 1:
输入: [2,2,1] 输出: 1  示例 2:
输入: [4,1,2,1,2] 输出: 4  实现 1 hash 使用字典存储元素以及元素出现次数，查询字典中出现次数为1的元素返回
时间复杂度O(n)
def num_only_once_dic(nums): &amp;quot;&amp;quot;&amp;quot; 使用额外空间实现 使用字典存储元素以及元素出现次数 查询字典中出现次数为1的元素 &amp;quot;&amp;quot;&amp;quot; nums_dic = {} for num in nums: if num in nums_dic: nums_dic[num] += 1 else: nums_dic[num] = 1 for vec in nums_dic: if nums_dic[vec] == 1: return vec return None  2 双指针 先排序
再使用双指针移动检查，指针移动步长为2
检查前后指针所指元素是否相同，不相同返回前指针所指元素
当检查到倒数第二组还为发现所选元素，那么所选元素一定为组后一个元素，跳出循环，直接返回最后一个元素
不占用额外空间
def num_only_once(nums): &amp;quot;&amp;quot;&amp;quot; 不使用额外空间实现 先排序 再使用双指针移动检查，指针移动步长为2 检查前后指针所指元素是否相同，不相同返回前指针所指元素 &amp;quot;&amp;quot;&amp;quot; pre_point = 0 check_point = 1 nums_len = len(nums) nums.</description>
    </item>
    
    <item>
      <title>leetcode刷题-54-螺旋矩阵</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-54-%E8%9E%BA%E6%97%8B%E7%9F%A9%E9%98%B5/</link>
      <pubDate>Fri, 25 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-54-%E8%9E%BA%E6%97%8B%E7%9F%A9%E9%98%B5/</guid>
      <description>问题描述 给定一个包含 m x n 个元素的矩阵（m 行, n 列），请按照顺时针螺旋顺序，返回矩阵中的所有元素。
示例 示例 1:
输入: [ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ] ] 输出: [1,2,3,6,9,8,7,4,5] 示例 2:
输入: [ [1, 2, 3, 4], [5, 6, 7, 8], [9,10,11,12] ] 输出: [1,2,3,4,8,12,11,10,9,5,6,7]
实现 1.模拟螺旋 def spiralOrder(self, matrix): def spiral_coords(r1, c1, r2, c2): for c in range(c1, c2 + 1): yield r1, c for r in range(r1 + 1, r2 + 1): yield r, c2 if r1 &amp;lt; r2 and c1 &amp;lt; c2: for c in range(c2 - 1, c1, -1): yield r2, c for r in range(r2, r1, -1): yield r, c1 if not matrix: return [] ans = [] r1, r2 = 0, len(matrix) - 1 c1, c2 = 0, len(matrix[0]) - 1 while r1 &amp;lt;= r2 and c1 &amp;lt;= c2: for r, c in spiral_coords(r1, c1, r2, c2): ans.</description>
    </item>
    
    <item>
      <title>leetcode刷题-945-使数组唯一的最小增量</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-945-%E4%BD%BF%E6%95%B0%E7%BB%84%E5%94%AF%E4%B8%80%E7%9A%84%E6%9C%80%E5%B0%8F%E5%A2%9E%E9%87%8F/</link>
      <pubDate>Fri, 25 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-945-%E4%BD%BF%E6%95%B0%E7%BB%84%E5%94%AF%E4%B8%80%E7%9A%84%E6%9C%80%E5%B0%8F%E5%A2%9E%E9%87%8F/</guid>
      <description>问题描述 给定整数数组 A，每次 move 操作将会选择任意 A[i]，并将其递增 1。
返回使 A 中的每个值都是唯一的最少操作次数。
示例 示例 1:
输入：[1,2,2] 输出：1 解释：经过一次 move 操作，数组将变为 [1, 2, 3]。 示例 2:
输入：[3,2,1,2,1,7] 输出：6 解释：经过 6 次 move 操作，数组将变为 [3, 4, 1, 2, 5, 7]。 可以看出 5 次或 5 次以下的 move 操作是不能让数组的每个值唯一的。
实现 实现1 字典存储元素及元素重复情况，重复的逐个进行计算 A_len: A数组长度，用于遍历
A_dic: 存储数组元素以及元素出现次数
repeates: 存储有重复的元素
repeate_done: 以完成递增操作的元素以及递增数值
move_sum: 递增操作总次数
将数组使用数据类型为字典的A_dic存储，数组元素对应key值，元素数量对应value值
遍历A_dic中出现次数大于1的元素，将其放入到repeates中
遍历repeates数组，开始进行递增操作，先检查是否已经进行过相同元素的操作，若有，则从上次相同元素的递增次数开始探测，若没有，则从该元素的下一位开始探测，若有空位，则move_sum加上探测成功的递增次数，若没有，继续进行下一位的探测，直至找到空位为止
时间复杂度为O(n^2)
def min_increament_for_unique(A): &amp;quot;&amp;quot;&amp;quot; hash存储，找相邻的空位置相加 &amp;quot;&amp;quot;&amp;quot; A_len = len(A) A_dic = {} repeates = [] repeate_done = {} move_sum = 0 for i in range(A_len): if A[i] in A_dic: A_dic[A[i]] += 1 repeates.</description>
    </item>
    
    <item>
      <title>leetcode刷题-11-盛最多水的容器</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-11-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/</link>
      <pubDate>Tue, 22 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-11-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/</guid>
      <description>问题描述 给定 n 个非负整数 a1，a2，&amp;hellip;，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。
说明：你不能倾斜容器，且 n 的值至少为 2。
示例 输入: [1,8,6,2,5,4,8,3,7] 输出: 49  实现 #### 1 暴力破解
暴力破解，时间复杂度$$O(n^{2})$$,超出官方指定时间限制，自己跑了一下大约是两秒半
def max_area(height_list): height_num = len(height_list) max_area = 0 for i in range(height_num): current_val = height_list[i] for j in range(i+1, height_num): if current_val &amp;lt;= height_list[j]: area = current_val * (j-i) else: area = height_list[j] * (j-i) if max_area &amp;lt; area: max_area = area return max_area`  2 双指针法 头尾指针 头尾指针，双向移动，每次移动较小值，时间复杂度$$O(n)$$</description>
    </item>
    
    <item>
      <title>leetcode刷题-16-最接近的三数之和</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-16-%E6%9C%80%E6%8E%A5%E8%BF%91%E7%9A%84%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/</link>
      <pubDate>Tue, 22 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-16-%E6%9C%80%E6%8E%A5%E8%BF%91%E7%9A%84%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/</guid>
      <description>问题描述 给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。
示例 给定数组 nums = [-1，2，1，-4], 和 target = 1.
与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2).
实现 双指针两侧扩展 双指针从目标值开始，同时向两侧扩展，取最先遇到的三个数相加
def three_sum_clostest(nums, target): &amp;quot;&amp;quot;&amp;quot; 给定一个包括n个整数的数组nums和一个目标值target。 找出nums中的三个整数,使得它们的和与target最接近。返回这三个数的和。 假定每组输入只存在唯一答案。 &amp;quot;&amp;quot;&amp;quot; result_sum = 0 pre_distance = {} if target&amp;lt;0: max_width = len(nums) - target else: max_width = len(nums) + target current_nums_num = 0 break_flag = False for x in nums: # print(x) current_dis = (target - x) if current_dis &amp;lt; 0: current_dis = - current_dis if pre_distance.</description>
    </item>
    
    <item>
      <title>leetcode刷题-26-删除有序数组重复项</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-26-%E5%88%A0%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E9%87%8D%E5%A4%8D%E9%A1%B9/</link>
      <pubDate>Tue, 22 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-26-%E5%88%A0%E9%99%A4%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E9%87%8D%E5%A4%8D%E9%A1%B9/</guid>
      <description>问题描述 给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。
不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成
示例 给定数组 nums = [1,1,2],
函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。
你不需要考虑数组中超出新长度后面的元素。
实现 双指针移动 设置主指针move_pointer和检查指针check_pointer,check_pointer永远在move_pointer后一位，若两个位置元素相等，则删掉数组move_pointer指向元素，数组长度减一，指针不动，继续判断，若两个位置元素不相等，则两指针同时往后移动一位，直至check_pointer移动到数组最后一位为止
def remove_duplicates(nums): &amp;quot;&amp;quot;&amp;quot; 不能使用额外数组 双指针移动 &amp;quot;&amp;quot;&amp;quot; reslut_len = len(nums) move_pointer = 0 check_pointer = 1 while (check_pointer != reslut_len and reslut_len != 0): if(nums[check_pointer] == nums[move_pointer]): del(nums[check_pointer]) reslut_len -= 1 else: move_pointer += 1 check_pointer += 1 return reslut_len nums = [1, 1, 2] result = remove_duplicates(nums) print(result)  总结 需要注意边界条件，输入数组可能为空值，在循环遍历时要注意数组为空的情况和索引的设置</description>
    </item>
    
    <item>
      <title>leetcode刷题-27-移除元素</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-27-%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/</link>
      <pubDate>Tue, 22 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-27-%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0/</guid>
      <description>问题描述 给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。
不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。
元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。
示例 给定 nums = [3,2,2,3], val = 3,
函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。
你不需要考虑数组中超出新长度后面的元素。
实现 单指针移动，当前值不等于目标值，指针向后移动一位，当前值等于目标值，指针不动，删掉当前元素，数组长度减一
def remove_element(nums, val): nums_len = len(nums) index = 0 while(index != nums_len): if(nums[index] == val): del(nums[index]) nums_len -= 1 else: index += 1 return nums_len nums = [3, 2, 2, 3] val = 3 result = remove_element(nums, val) print(result)  总结 使用while循环往往比for循环要好一些，边界值判断、索引值设定更加简单</description>
    </item>
    
    <item>
      <title>深度学习-推荐博文</title>
      <link>https://okery.github.io/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E5%8D%9A%E6%96%87/</link>
      <pubDate>Mon, 21 Oct 2019 10:20:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%8E%A8%E8%8D%90%E5%8D%9A%E6%96%87/</guid>
      <description>神经网络浅讲：从神经元到深度学习
https://www.cnblogs.com/subconscious/p/5058741.html</description>
    </item>
    
    <item>
      <title>leetcode刷题-1-两数相加</title>
      <link>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-1-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</link>
      <pubDate>Sun, 20 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/leetcode%E5%88%B7%E9%A2%98-1-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</guid>
      <description>立个flag，研二去北京实习，去找小仙女吖！
问题描述 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那两个整数，并返回他们的数组下标。
你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。
示例 给定 nums = [2, 7, 11, 15], target = 9
因为 nums[0] + nums[1] = 2 + 7 = 9 所以返回 [0, 1]
实现 1 暴力破解 首先想到的是暴力破解，也能跟后面其他方法对比一下复杂度
def two_sum_vio(nums, target): num_len = len(nums) results = [] for i in range(0, num_len): flage = False remainder = target - nums[i] for j in range(0, num_len): if(nums[j] == remainder and j != i): results.</description>
    </item>
    
    <item>
      <title>机器学习笔记-朴素贝叶斯</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</link>
      <pubDate>Fri, 11 Oct 2019 15:37:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</guid>
      <description>分类问题 定义 从数学角度来看，分类问题可做如下定义：
集合:$$C= \left{ y{1},y{2},&amp;hellip;y{n} \right}, I= \left{ x{1}, x{2}, &amp;hellip;,x{m},&amp;hellip; \right}$$
确定映射规则：y=f(x)
对于任意x属于I，有且仅有一个y属于C，使得y=f(x)成立
其中：C为类别集合， I为项集合， f为分类器
分类算法的任务就是构造分类器f
补充 分类型问题往往采用经验性方法构造映射规则，即一般情况下的分类问题缺少足够的信息来构造100%正确的映射规则。
贝叶斯定理 贝叶斯定理解决了一个问题 已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)情况下，如何求得P(B|A)
条件概率 $$P(A|B)=\frac{P(AB)}{P(B)}$$
贝叶斯定理公式 $$P(B|A)=\frac{P(A|B)P(B)}{P(A)}$$
朴素贝叶斯 原理 对于给出的待分类项，求解在此出现的条件下各个类别出现的概率，那个最大，就认为此待分类项属于哪个类别
正式定义  $$设x=\left{a{1}, a{2}, &amp;hellip;, a{m}\right}为一个待分类项，a{i}为x的一个属性$$ $$有类别集合：c=\left{y{1}, y{2}, &amp;hellip;, y_{n}\right}$$ $$计算p(y{1}|x), p(y{2}|x), &amp;hellip;, p(y_{n}|x)$$ $$若p(y{k}|x)=max \left{ p(y{1}|x), p(y{2}) \right}，那么x\in y{k}$$  案例代码 # coding=utf8 &amp;quot;&amp;quot;&amp;quot; @File : bayesain_action.py @Contact : 13132515202@163.com @Modify Time @Author @Version @Description ------------ ------- -------- ----------- 2019/10/11 20:56 LiuHe 1.</description>
    </item>
    
    <item>
      <title>神经网络笔记-神经网络基础</title>
      <link>https://okery.github.io/post/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Mon, 07 Oct 2019 10:20:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0/</guid>
      <description>### 神经元模型
神经元是脑组织的基本单位，是神经系统结构与功能的单位。
M-P模型 M-P模型通过对生物神经元信息处理过程进行了简化和概括
 多个输入单个输出 不同输入权重不同 多输入累加整合 阈值特征  模型是把神经元是为二值开关元件，按照不同方式组合来完成各种逻辑运算。能够构成逻辑与、非、或，理论上可以进而组成任意复杂的逻辑关系，若将M-P模型按一定方式组织起来，可以构成具有逻辑功能的神经网络
​ $$net{j}=\sum{i=1}^{n}w{ij}x{i}$$
$$net_{j}$$为累加代数和
​ $$o{j}=f(net{j}-T{j})=f(\sum{i=1}^{n}w{ij}-T{j})$$
$$T_{j}$$为阈值
激活函数 也叫连接函数、传递函数、变换函数或者激励函数。用来模拟神经元输出与其激活状态之间的联系：输入达到某个阈值后达到激活状态，否则为抑制肽。不同的激活函数，会使神经元具有不同的信息处理特性。对于神经网络来讲，激活函数的主要作用就是进行线性变换，增加系统的非线性表达函数
常见激活函数 Sign函数：$$f(x)=sgn(x)$$
sigmoid函数：$$f(x)=\frac{1}{1+e^{-x}}$$(单极性S函数)
Tanh函数：$$f(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$(双极性S函数)
Arctan函数：$$f(x)=arctan(x)$$
神经网络模型分类 按照拓扑结果可分为层次结果和互连结构 层次结构  单纯层次结构 层内有互连 输出层到输入层有连接  互连结构  全互联：每个节点和其他所有节点连接 局部互联：每个节点只与其邻近节点有连接 稀疏连接：节点只与少数相距较远的节点有连接  按照信息流量可分为前馈性网络和反馈型网络 前馈型网络 网络信息从输入层到各隐藏层再到输出层逐层前进
采用单项多层结构，各神经元分层排列，每个神经元只与前一层的神经元项链。接收前一层的输出，并输出给下一层，各层间没有反馈。
包含三类节点
 输入节点：外界信息输入，不进行任何计算，仅向下一层节点传递信息，此层必须有 隐藏节点：接受上一层节点的输入，进行结算，并将信息传到下一层节点，此层可以没有，没有即为单层感知器 输出节点：接收上一层节点的输入，进行计算，并将结果输出，此层必须有  #### 反馈型网络
反馈网络中所有节点都具有信息处理能力，并且每个节点既可以接受输入同时又可以进行输出
神经网络学习规则 有监督学习：学习模式为纠错 无监督学习：学习模式为自组织 灌输式学习：学习模式为死记硬背 赫布学习规则 赫布学习规则为前馈、无导师学习。只根据实际输入和输出调整权重
在赫布学习规则中，学习信号简单的等于神经元的输出：$$r=f(W_{j}^{T}X)$$
权值向量的调整公式为：$$\Delta W{j}=\eta f(W{j}^{T}X)X, \eta 为常数$$
权向量各个分量调整为：$$\Delta w{ij}=\eta f(W{j}^{T}X)x{i}=\eta o{j}x_{j},i=0,1,&amp;hellip;,n$$
赫布学习规则的步骤：</description>
    </item>
    
    <item>
      <title>机器学习笔记-决策树</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Sat, 05 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>优缺点 ​ 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
​ 缺点：可能产生过度匹配问题
​ 适用数据类型：数值型和标称型
创建分支伪代码createBranch() ​ 检测数据集中每个子项是否属于同一分类
​ IF so return 类标签
​ ELSE
​ 寻找划分数据集的最好特征
​ 划分数据集
​ 创建分支节点
​ for 每个划分的子集
​ 调用函数createBranch并增加返回结果到分支节点中
​ return 分支节点
一般流程 收集数据：可以使用任何方法
准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
分析数据：可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期
训练算法：构造书的数据结构
测试算法：使用经验树计算错误率
使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好的理解数据的内在含义
信息增益 信息的定义 如果待分类的事务可能划分在多个分类之中，则符号$$x_{i}$$的信息的定义为
$$l(x{i})=-log{2}p(x_{i})$$
其中$$p(x_{i})$$是选择该分类的概率
熵的定义 熵为信息的期望值，在信息论和概率统计中，熵是表示随机变量不确定性的变量。设X是一个取有限个值的离散随机变量，其概率分布为
$$P(X=x{i}) = p{i}. i=1,2,&amp;hellip;,n$$
则随机变量X的熵定义为：
​ $$H=-\sum{i=1}^{n}p(x{i})log{2}p(x{i})$$
熵越大，随机变量的不确定性就越大
条件熵 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望
$$H(Y|X)=\sum{i=1}^{n}p{i}H(Y|X=x_{i})$$
这里，pi=P(X=xi), i=1,2,&amp;hellip;,n
信息增益 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度
特征A对训练数据集D的信息增益g(D, A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即
g(D,A)=H(D)-H(D|A)
信息增益大的特征具有更强的分类能力，所以在构建决策树时首先选择信息增益最大的特征作为子节点
决策树工作原理 得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个几点上，我们再次划分数据&amp;hellip;&amp;hellip;
所以使用递归的原则来处理数据集
递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类
决策树的生成 ID3算法 输入：训练数据集D，特征集A，阈值δ</description>
    </item>
    
    <item>
      <title>机器学习笔记-k近邻算法距离度量</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k%E8%BF%91%E9%82%BB%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/</link>
      <pubDate>Tue, 01 Oct 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k%E8%BF%91%E9%82%BB%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/</guid>
      <description>k近邻算法中的距离度量规则 特征空间中两个实例点的距离是两个实例点相似程度的反映。
设特征空间X是n维实数向量空间$$R^{n},x{i},x{j}\in X,x{i}=(x{i}^{1},x{i}^{2},&amp;hellip;,x{i}^{n})^{T},x{j}=(x{j}^{1},x{j}^{2},&amp;hellip;,x{j}^{n})^{T}$$
则$$x{i},x{j}的L_{p}距离定义为$$
$$L{p}(x{i},x{j})=(\sum{l=1}^{n}|x{i}^{(l)}-x{j}^{l}|^{p})^{\frac{1}{p}}$$
这里p&amp;gt;=1.
欧式距离 当上述p=2时，称为欧氏距离，即
$$L{2}(x{i},x{j})=(\sum{l=1}^{n}|x{i}^{(l)}-x{j}^{l}|^{2})^{\frac{1}{2}}$$
曼哈顿距离 当上述p=1时，称为曼哈顿距离，即
$$L{1}(x{i},x{j})=(\sum{l=1}^{n}|x{i}^{(l)}-x{j}^{l}|)$$
距离最大值 当p趋向无穷时，它是各个坐标距离的最大值，即
$$L{\infty}(x{i},x{j})=max{l}|x{i}^{(l)}-x{j}^{l}|$$</description>
    </item>
    
    <item>
      <title>python学习笔记-map、zip函数</title>
      <link>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-mapzip%E5%87%BD%E6%95%B0/</link>
      <pubDate>Mon, 30 Sep 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-mapzip%E5%87%BD%E6%95%B0/</guid>
      <description>map函数 map函数会根据提供的函数对指定序列做映射， 第一个参数function一参数序列中的每一个元素调用function函数，返回包含每次function函数返回值的新列表 将数组中int类型变量转换成str类型
a = [1, 2, 3] print(list(map(str, a)))  输出为：
[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;]  也可以对数组进行算数操作, 对a中每个元素进行加法操作
a = [ [1, 2, 3], [4, 5, 6] ] print(list(map(sum, a)))  输出
[6, 15]  使用lambda表达式
a = [1, 2, 3, 4, 5] print(list(map(lambda x: x**2, a)))  输出
[1, 4, 9, 16, 25]  zip函数 zip函数用于将可迭代的对象作为参数，将对象照片对应的元素打包成一个个元组，然后返回由这些元组组成的列表 若各个迭代器的元素个数不一致，则返回列表长度与最短的对象响应， 利用*操作符，可以将元组解压为列表 在python3.x中，为了减少内存，zip函数返回的是一个对象。如需展示列表，需要手动list()进行转换 将对应位置打包成元组的列表
a = [1, 2, 3] b = [4, 5, 6] print(list(zip(a, b)))  输出</description>
    </item>
    
    <item>
      <title>python学习笔记-小数取整</title>
      <link>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%8F%E6%95%B0%E5%8F%96%E6%95%B4/</link>
      <pubDate>Mon, 30 Sep 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%B0%8F%E6%95%B0%E5%8F%96%E6%95%B4/</guid>
      <description>取整函数 向下取整 直接使用内置的int()
四舍五入 对数字进行四舍五入使用round()
向上取整 使用math模块中的ceil()方法
分别取整数部分和小数部分 使用math模块中的modf()方法，该方法返回一个包含小数部分和整数部分的元组</description>
    </item>
    
    <item>
      <title>机器学习笔记—K-近邻算法</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 30 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid>
      <description>原理概述 ​ 存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本中特征最相似数据的分类标签，我们只选择样本数据集中前k个最相似的数据，选择k个最相似数据中出现次数最多的分类，作为新数据的分类
k-近邻算法的流程 收集数据：可以使用任何方法
准备数据：距离计算所需要的数值，最好是结构化的数据格式
分析数据：可以使用任何方法
训练算法：此步骤不适应k-近邻算法
测试算法：计算错误率
使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于那个分类，最后应用对计算出的分类执行后续的处理。
KNN分类算法实现步骤 对未知类别属性的数据集中的每个点依次执行以下操作：
1、计算已知类别数据集中的点与当前点之间的距离；
2、按照距离递增次序排序；
3、选取与当前点距离最小的k个点；
4、确定前k个点所在类别的出现频率；
5、返回前k个点出现频率最高的类别作为当前点的预测分类。
KNN分类算法的优缺点 优点 ​ 精度高、对异常值不敏感、无数据输入假定
缺点 ​ 计算复杂度高、空间复杂度高
使用数据范围 ​ 数值型和标称型
总结 ​ 明天国庆节，祝伟大的祖国生日快乐。</description>
    </item>
    
    <item>
      <title>机器学习笔记-逻辑回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 23 Sep 2019 15:37:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</guid>
      <description>模型原理 分类变量 ​ 又称定性变量、离散型变量，观测的个体智能属于几种互不相容的类别的一种，一般用非数字来表达其类别，这种数据被称为分类变量。与之相对的是连续变量，即定量变量，是由测量、技术或者统计等得到的，这些变量具有数值特征
#### 常见的分类变量
​ 有序变量：没有数量关系，但有次序关系。比如：收入等级、客户级别
​ 名义变量：即无等级关系，也无数量关系。比如：性别、天气等
因变量为分类变量的特点 ​ 误差项非正态分布：误差分布为两点型离散分布，不是正态分布
​ 误差项零均值异方差：均值为0，误差的方差依赖于自变量，会随着自变量的不同水平而变化，是异方差
​ 回归方程限制：y为0-1型随机变量
改进 ​ 回归方程改用限制在[0,1]之间的连续曲线，而不再是直线回归方程，常用的是Logisitic函数
​ 因变量取值为0，1，不适合直接作为回归的因变量，改用因变量取1的概率Π作为回归因变量</description>
    </item>
    
    <item>
      <title>算法学习笔记-杂记</title>
      <link>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9D%82%E8%AE%B0/</link>
      <pubDate>Mon, 23 Sep 2019 14:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9D%82%E8%AE%B0/</guid>
      <description>此笔记记录在学习过程中想到的或者从其余方法学中获取到的一些算法思想或者比较有用的信息，记录在此。 比值使用 在进行数据分析时，可以使用比值来判断个体在某种方面的效率大小
来源自背包问题，可以计算物品的价值重量比，来确定选择物品的优先级，可以得到最优解</description>
    </item>
    
    <item>
      <title>概率论笔记-常用统计量及其意义</title>
      <link>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%BB%9F%E8%AE%A1-%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1%E9%87%8F%E5%8F%8A%E5%85%B6%E6%84%8F%E4%B9%89/</link>
      <pubDate>Sun, 22 Sep 2019 21:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%BB%9F%E8%AE%A1-%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1%E9%87%8F%E5%8F%8A%E5%85%B6%E6%84%8F%E4%B9%89/</guid>
      <description>统计量 定义 设X1, X2, &amp;hellip;, Xn是来自总体X的一个样本，g(X1, X2, &amp;hellip;, Xn)是X1，X2， &amp;hellip;, Xn的函数，若g中不含位置参数，则称g(X1, X2, &amp;hellip;, Xn)是以统计量
常用统计量 1.样本均值 $$\bar{X}=\frac{1}{n}\sum{X{i}}, 其观察值为\bar{x}=\frac{1}{n}\sum{x{i}}$$
统计意义：描述数据取值的平均位置
2.样本方差 $$S^{2}=\frac{1}{n-1}\sum{i=1}^{n}(X{i}-\bar{X})^{2}=\frac{1}{n-1}(\sum{i=1}^{n}X^{2}{i}-n\bar{X}^{2}),其观察值为s^{2}=\frac{1}{n-1}\sum{i=1}^{n}(x{i}-\bar{x})^{2}=\frac{1}{n-1}(\sum{i=1}^{n}x^{2}{i}-n\bar{x}^{2})$$
统计意义：方差用来计算每一个变量与总体均数之间的差异
概率意义：用来度量随机变量与数学期望之间的偏离程度
3.样本标准差 $$\sqrt{\frac{1}{n-1}\sum{i=1}^{n}(X{i}-\bar{X})^{2}}=\sqrt{\frac{1}{n-1}(\sum{i=1}^{n}X^{2}{i}-n\bar{X}^{2})}, 其观察值为s=\sqrt{\frac{1}{n-1}\sum{i=1}^{n}(x{i}-\bar{x})^{2}}=\sqrt{\frac{1}{n-1}(\sum{i=1}^{n}x^{2}{i}-n\bar{x}^{2})}$$
统计意义：标准差定义是总体各单位标准值与其平均数离差平方的算数平均数的平方根，它反映组内个体间的离散程度，一个较大的标准差，代表大部分数值和其平均值之间差异较大， 一个较小的标准差，代表这些数值较接近平均值
感谢 https://blog.csdn.net/Hearthougan/article/details/77859173</description>
    </item>
    
    <item>
      <title>python学习笔记-浅拷贝、深拷贝</title>
      <link>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B5%85%E6%8B%B7%E8%B4%9D%E6%B7%B1%E6%8B%B7%E8%B4%9D/</link>
      <pubDate>Sat, 21 Sep 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B5%85%E6%8B%B7%E8%B4%9D%E6%B7%B1%E6%8B%B7%E8%B4%9D/</guid>
      <description>前述 python中的浅拷贝、深拷贝和Java里面的概念是一样的，浅拷贝就是对引用的拷贝，深拷贝就是对对象的资源的赋值是将一个对象的地址赋值给一个变量，让变量指向该地址
修改不可变对象需要开辟新空间
修改可变对象不需要开辟新空间
浅拷贝 浅拷贝仅仅是赋值元素的地址</description>
    </item>
    
    <item>
      <title>杨辉三角</title>
      <link>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9D%A8%E8%BE%89%E4%B8%89%E8%A7%92/</link>
      <pubDate>Sat, 21 Sep 2019 14:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9D%A8%E8%BE%89%E4%B8%89%E8%A7%92/</guid>
      <description>杨辉三角
python实现输出指定行数的杨辉三角 # -*- coding: utf-8 -*- # @Date : 2019-09-20 14:22:58 # @Author : LiuHe (liuh131@163.com) # @Link : https://okery.github.io/ # @Version : $Id$ # @Name : 生成指定行数的杨辉三角 # 生成杨辉三角 def triangle(row): # 使用二维数组存储内容 trianles = [] for j in range(row): triangle_inner = [] for i in range(j): if i == 0 or i == (j - 1): triangle_inner.append(1) else: tmp = trianles[j - 1][i - 1] + trianles[j - 1][i] triangle_inner.</description>
    </item>
    
    <item>
      <title>概率论笔记-T检验</title>
      <link>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%AC%94%E8%AE%B0-t%E6%A3%80%E9%AA%8C/</link>
      <pubDate>Fri, 20 Sep 2019 21:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%AC%94%E8%AE%B0-t%E6%A3%80%E9%AA%8C/</guid>
      <description>假设检验-T检验
假设检验概念 ​ 假设检验也叫显著性检验，是以小概率反证法的逻辑推理，判断假设是否成立的统计方法，它首先假设样本对应的总体参数（或分布）与某个已知总体参数（或分布）相同，然后根据统计量的分布规律来分析样本数据，利用样本信息判断是否支持这种假设，并对检验假设做出取舍抉择，做出的结论是概率性的，不是觉得的肯定和否定。
T检验 ​ T检验是用于两个样本平均值差异程度的检验方法。
​ T检验的适用条件为样本分布符合正态分布
​
​</description>
    </item>
    
    <item>
      <title>概率论笔记-最小二乘法</title>
      <link>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</link>
      <pubDate>Fri, 20 Sep 2019 21:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/</guid>
      <description>二乘的意思就是平方，最小二乘法也叫最小平方法，通过最小化误差的平方和寻找数据的最佳函数匹配</description>
    </item>
    
    <item>
      <title>python学习笔记-list列表</title>
      <link>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-list%E5%88%97%E8%A1%A8/</link>
      <pubDate>Fri, 20 Sep 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-list%E5%88%97%E8%A1%A8/</guid>
      <description>list列表 list是有序集合，没有固定大小，根据数据量的不同可以动态变化，索引从0开始
访问元素 使用索引访问元素
classmates = [&#39;lisi&#39;, &#39;zhangsan&#39;] # 取 lisi， 索引为0 print(classmates[0])  若要取最后元素，可以使用-1做索引, 依次类推，可以用-2取倒数第二个元素, &amp;hellip;&amp;hellip;
classmates = [&#39;lisi&#39;, &#39;zhangsan&#39;] # 取 zhangsan，索引使用-1 print(classmates[-1])  index(), 返回列表中第一个值为x的元素的索引，若没有，则返回一个错误
classmates = [&#39;lisi&#39;, &#39;zhangsan&#39;] lisi_index = classmates.index(&#39;lisi&#39;) print(lisi_index)  统计列表中元素x出现的次数
classmates = [&#39;lisi&#39;, &#39;zhangsan&#39;] lisi_count = classmates.count(&#39;lis&#39;) print(lisi_count)  遍历列表 方法1
classmates = [&#39;lisi&#39;, &#39;zhangsan&#39;] for i in classmates: print(&amp;quot;索引：%s 值：%s&amp;quot; % (classmates.index(i), i))  方法2
classmates = [&#39;lisi&#39;, &#39;zhangsan&#39;] for i in range(len(classmates)): print(&#39;索引：%s 值：%s&#39; % (i, classmates[i]))  方法3</description>
    </item>
    
    <item>
      <title>算法学习笔记-欧几里得算法实现</title>
      <link>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</link>
      <pubDate>Fri, 20 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</guid>
      <description></description>
    </item>
    
    <item>
      <title>概率论笔记-最大似然估计</title>
      <link>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</link>
      <pubDate>Fri, 20 Sep 2019 09:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%AC%94%E8%AE%B0-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</guid>
      <description>最大似然估计
原理 ​ 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果退出参数的大概值。思想是：已知某个参数从能使这个样本出现的概率最大，那么就干脆将这个参数作为估计的真实值
​ 通俗点解释，就是认为如果某个事件发生了，那么它就必然是概率最大的。通过事实反过来猜测概率情况，就叫似然，通过事实，推断出最有可能的情况，就是最大似然估计。</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（三）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Thu, 19 Sep 2019 19:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid>
      <description>机器学习
线性回归分析 回归 ​ 回归是处理两个或两个以上变量之间互相依赖的定量关系的一种统计方法和技术，变量之间的关系并非确定的函数关系，通过一定的概率分布来描述
线性 ​ 线性的严格定义是一种映射关系，其映射关系满足可加性和其次性。通俗理解就是两个变量之间存在一次方函数关系，在平面坐标系中表现为一条直线。不满足线性即为非线性。
线性回归 ​ 线性回归：在回归分析中，如果自变量和因变量之间存在着线性关系，则被称为线性回归。如果只有一个因变量一个自变量，则称为一元线性回归，如果有一个因变量多个自变量，则被称作多元回归。
回归模型 ​ 回归模型的一般形式是：
​ $$y=f(x{1},x{2},x{3},&amp;hellip;.,x{p})+\varepsilon$$
​ f()函数为确定性关系，后者为随机误差
建立回归模型 流程 ​ 1.需求分析明确变量：了解相关需求，明确场景，清楚需要解释的指标（因变量），并根据相关业务知识选取与之有关的变量作为解释变量（自变量）
​ 2.数据收集加工：根据上一步分析得出的解释变量，去收集相关的数据（时序数据、截面数据等），对得到的数据进行清洗、加工，并根据数据情况调整解释变量，并判断是否满足基本假设。
​ 3.确定回归模型： 了解数据集，使用绘图工具绘制变量样本散点图或使用其他分析工具分析变量间的关系，根据结果选择回归模型，如：线性回归模型、指数形式的回归模型等。
​ 4.模型参数估计：模型确定后，基于收集、整理的样本数据，估计模型中的相关参数。最常用的方法是最小二乘法，在不满足基本假设的情况下还会采取岭回归、主成分回归、偏最小二乘法等
​ 最小二乘法：也叫做最小平方法，通过最小化误差的平方和寻找数据的最佳函数匹配的方法
​ 5.模型检验优化：参数确定后，得到模型。此时需要对模型进行统计意义上的检验，包括对回归方程的显著性检验、回归系数的显著性检验、拟合优度检验、异方差检验、多重共线性检验等。还要结合实际场景，判断该模型是否具有意义。
​ 6.模型部署应用
回归模型的特点 ​ 模型简单，建模和应用都比较容易
​ 由坚实的统计理论支撑
​ 定量分析各变量之间的关系
​ 模型预测结果可以通过误差分析精确了解
​ 但：
​ 假设条件比较多且相对严格
​ 变量选择对模型影响较大</description>
    </item>
    
    <item>
      <title>算法学习笔记-筛选质数算法</title>
      <link>https://okery.github.io/post/%E7%AD%9B%E9%80%89%E8%B4%A8%E6%95%B0%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 18 Sep 2019 20:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AD%9B%E9%80%89%E8%B4%A8%E6%95%B0%E7%AE%97%E6%B3%95/</guid>
      <description>筛选算法——筛选指定范围内的质数
案例 指定范围为：1-20
数组为：
​ 1 2 3 4 5
​ 6 7 8 9 10
​ 11 12 13 14 15
​ 16 17 18 19 20
​ 21 22 23 24 25
1.去掉2的倍数
2.去掉3的倍数
3.去掉4的倍数
4.去掉5的倍数
去掉方法为：
​ j = i * i
​ j = j + i
​ i二次幂肯定为i的倍数，所以不为质数，去掉
​ 从i * i 处开始，以i为步长往后对应的数肯定为i的倍数，所以不为质数，去掉
​ 本案例范围为1-25，所以最高去掉5的倍数即可
算法实现 # 筛选算法 # 筛选给定范围内的质数 # 给定范围，将数据存放进数组，将非质数所在位置设为0，返回数组 def find_prime_number(a): numbers = [] # 初始化数组，将数据填充进数组 for i in range(1, a+1): numbers.</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（一）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%80/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%80/</guid>
      <description>机器学习第零章  
机器学习流程  输入数据 特征工程 模型训练 模型部署 模型应用  相关基本概念 输入空间 ​ 将输入的可能取值的集合称作输入控件
输出空间 ​ 将输出的所有可能取值的集合称作输出空间
特征空间 ​ 特征：即属性。每个输入实例的各个组成部分称作原始特征，基于原始特征还可以扩展出更多的衍生特征
​ 特征向量：由多个特征组成的集合，称作特征向量
​ 特征空间：将特征向量存在的空间称作特征空间
假设空间 ​ 假设空间：由输入空间到输出空间的映射的集合，称作假设空间
机器学习方法三要素 方法 = 模型 + 策略 + 算法 模型： ​ 输入空间到输出空间的映射关系
策略： ​ 从假设空间众多的假设中选择到最有的模型的学习标准或规则
算法： ​ 学习模型的具体的计算方法，通常是求解最优化问题
模型分类 ​ 预测分类：分类 Classification
​ 预测取值：回归 Regression
​ 发现结构：聚类 Clustering
​ 发现异常数据：异常检测 Anomaly Detection
损失函数 ​ 用来衡量预测结果和真实结果之间的差距，其值越小，代表预测结果和真实结果越一致
​ 平方损失函数：预测结果与真实结果差的平方
​ $$ L(Y,f(x)) = (Y - f(x))^{2} $$</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（二）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BA%8C/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BA%8C/</guid>
      <description>机器学习笔记 
模型选择的原则 误差 ​ 是模型的预测输出值与其真实值之间的差异
训练 ​ 通过已知的样本数据进行学习，从而得到模型的过程
训练误差 ​ 模型作用与训练集时的误差
泛化 ​ 有具体的、个别的扩大为一般的，即从特殊到一半，成为泛化。对机器学习的模型来讲，泛化是指模型作用于新的样本数据（非训练集）
泛化误差 ​ 模型作用于新的样本数据时的误差
模型容量 ​ 是指拟合各种模型的能力
过拟合和欠拟合 ​ 是某个模型在训练集上表现很好，但是在新样本上表现差。模型将训练集的特征学习的太好，导致一些非普遍规律被模型接纳和体现，从而在训练集上表现好，但是对于新样本表现差。反之则成为欠拟合，即模型对训练集的一边性质学习较差，模型作用于训练集时表现不好
模型选择 ​ 针对某个具体的任务，通常会有多种模型可供选择，对同一个模型也会有多组参数，可以通过分析、评估模型的泛化误差，选择泛化误差最小的模型
模型的评估方法 评估思路 ​ 通过实验测试，对模型的泛化误差进行评估，选出泛化误差最小的模型。待测数据集全集未知，使用测试集进项泛化测试，测试误差即为泛化误差的近似
​ 测试集和训练集尽可能互斥
​ 测试集和训练集独立同分布
留出法 ​ 将已知数据集的划分极可能保持数据分布一致性，避免因数据划分过程引入为的偏差
​ 数据分割存在多种形式会导致不同的训练集、测试集划分，单次留出法结果往往存在偶然性，其稳定性较差，通常会进行若干次随机划分、重复实验评估取平均值作为评估结果
​ 数据集拆成两部分，每部分的规模设置会影响评估结果，测试、训练的比例通常为7：3
​ 测试集和训练集分开，缓解了过拟合，但一次划分，评估结果偶然性大，而且数据被拆分后，用户训练、测试的数据更小了
交叉验证法 ​ 将数据集划分k个大小相似的互斥的数据子集，子集数据尽可能保证数据分布的一致性（分层采样），每次从中选取一个数据集作为测试集，其余用作训练集，可以进行k次训练和测试，得到评估均值。该验证方法也称作k折交叉验证。使用不同的划分，重复p次，称作p次k折交叉验证
留一法 ​ 是k折交叉验证的特殊形式，将数据集分成两个，其中一个数据集记录条数为1，作为测试集使用，其余记录作为训练集模型。训练出的模型使用全部诗句集训练得到的模型接近，其评估结果比较准确。缺点是当数据集较大时，训练次数和规模较大。
​ 充分利用了所有样本，多次划分，评估结果相对稳定，但是计算比较繁琐，需要进行k次训练和评估
自助法 ​ 是一种产生样本的抽象方法，其实质是有放回的随机抽样。级从已知数据集中随机抽取一条数据，然后将该记录放入测试集同时放回原数据集，继续下一次冲一样，知道测试集中的数据条数满足要求。
​ 对总体的理论分布没有要求，但是无放回抽样引入了额外的偏差
几种方法的选择 ​ 已知数据集数量充足时，通常采用留出法或者k折交叉验证法
​ 对于已知数据集较小且难以有效划分训练集/测试集的时候，采用自助法
​ 对于已知数据集较小且可以有效划分训练集/测试集的时候，采用留一法
假设检验步骤 1.建立假设 ​ 根据具体的问题，建立假设</description>
    </item>
    
    <item>
      <title>算法学习笔记-欧几里得算法实现</title>
      <link>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</guid>
      <description> 欧几里得算法 欧几里得算法用来计算两个整数的最大公约数
公式 ​ gcd(a, b) = gcd(b, a mod b)
原理 ​ 两个整数的最大公约数等于其中较小的那个数和两数相除的最大公约数
实现 ​ 递归实现
def gcd_recursion(a, b): if b == 0: return a return gcd_re(b, a % b) print(gcd_re(6, 4))  ​ 非递归实现
def gcd_no_recursion(a, b): while b != 0: tmp = a % b a = b b = tmp return a  </description>
    </item>
    
    <item>
      <title>吴恩达机器学习——单变量线性回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>吴恩达机器学习——单变量线性回归
基本概念 ### 假设函数  ​ 我们得到一个训练集，并用其进行某种预测时，我们可以根据训练集得出一个假设函数。
​ 如下图所示，下图为某种行业效益值y与程视人口数量x的训练集
​ 根据图像特征，我们可以得出这些数据可以用下图函数来代表：
​ ​ 因此我们可以得出一个假设函数：
​
$$ h{\theta}=\theta{0}+\theta_{1}X $$ ​
代价函数 ​ 在作出假设函数之后，我们要对建设函数中的两个未知量$$\theta{0}$$和$$\theta{1}$$做出选择，即选择不同的$$\theta{0}$$$$\theta{1}$$，验证所选择的值是否能时假设函数最大程度的能跟数值相符合
​ 对于如何判断所选择的$$\theta{0}$$和$$\theta{1}$$时最好的，我们选择均方误差来作为衡量标准，即： $$ J(\theta)=\tfrac{1}{2m}\sum{i=0}^{m}(h{\theta}(x^{i})-y^{i})^{2} $$ ​ 此即为我们的代价函数，函数值绝对值越小表示$$\theta{0}$$和$$\theta{1}$$越符合目标值
梯度下降算法 前言 ​ 在求代价函数的最小值时，以二维图像来为例表示函数值变化规律来讲，假设函数值变化规律如下图所示
​ 1处为最小值点，我们发现如果根据函数特征看，当$$\Delta x$$与$$y&amp;rsquo;$$符号相反时，沿着这个方向可以从任何点到达最小值处，因此在学习过程中可以根据导数（当为多元函数时为梯度）来判断如何改变变量值，来是的求解过程更加快速。
梯度定义 ​ 梯度表示某一函数在该处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大
多元函数梯度表达式 $$ \bigtriangledown f = (\frac{\partial f}{\partial x{i}}) 其中(\bigtriangledown f){i}=\frac{\partial f}{\partial x_{i}} $$
根据梯度下降法则选择$$ \theta$$ ​ 在选取变量$$\theta$$值时，根据梯度下降法则应该按照以下规则选取$$\theta$$的值
​
$$ \theta = \theta - \alpha \bigtriangleup f $$ ​ 其中$$\alpha$$为学习率，自己选取,因为梯度的方向是函数f增长最快的方向，梯度的反方向是f降低最快的方向，所以选取梯度的负数方向。
梯度下降算法求解过程 ​ 1.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习——监督学习和无监督学习</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</guid>
      <description>吴恩达机器学习——监督学习和无监督学习 监督学习 特征：  有标签数据 2.直接反馈 3.预测结果/未来  对每个数据都有准确的反馈
两个子类 1.回归：Regression 预测结果为连续值即为回归
2.分类：Classification 预测结果为离散值即为分类
无监督学习 特征： 1.无标签/目标 2.无反馈 3.寻找数据中移仓的结构
两个子类 1.聚类： 见数据按相似度聚类成不同分组
2.降维： 在保留数据结构和有用性的同时对数据进行压缩</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(matlab)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</guid>
      <description>吴恩达机器学习——单变量线性回归作业（matlab） 1.Simple Octave/MATLAB function warmUpExercise.m：生成一个5 * 5 的单位矩阵 实现代码：
A = eye(5)	 笔记:matlab中eye(n)函数生成一个n*n的单位矩阵
结果：
2.Plotting the Data plotData.m：绘制数据，将数据集绘制在二维空间 实现代码:
plot(x,y,&#39;rx&#39;,&#39;MarkerSize&#39;,10); %Plot the data ylabel(&#39;Profit in $10,000s&#39;);	%set the y-axis label xlabel(&#39;Pupulation of City in 10,000s&#39;); %set the x-axis label  笔记：plot(x,y,&amp;lsquo;rx&amp;rsquo;,&amp;lsquo;MarkerSize&amp;rsquo;,10)参数含义:x、y为点的坐标，rx表示用红色的X形状来表示点，MarkerSize、10表示X形状的大小
结果：
​ ​
3.Computing the cost J(theta) computeCost.m：计算代价函数值 假设函数： $$ h{\theta}(x)=\theta^{\tau}x=\theta{0}+\theta{1}x $$ 代价函数： $$ J(\theta)=\tfrac{1}{2m}\sum{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^{2} $$ 实现代码：
J = sum(((X * theta) - y) .^ 2)/(2*m);  笔记：.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(python)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</guid>
      <description>机器学习—单变量线性回归作业(python) python代码实现链接： 代码</description>
    </item>
    
  </channel>
</rss>