<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on LiuHe&#39;s blog</title>
    <link>https://okery.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on LiuHe&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2019 15:37:58 +0800</lastBuildDate>
    
	<atom:link href="https://okery.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>机器学习笔记-朴素贝叶斯</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</link>
      <pubDate>Fri, 11 Oct 2019 15:37:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</guid>
      <description>分类问题 定义 从数学角度来看，分类问题可做如下定义：
集合:$$C= \left{ y{1},y{2},&amp;hellip;y{n} \right}, I= \left{ x{1}, x{2}, &amp;hellip;,x{m},&amp;hellip; \right}$$
确定映射规则：y=f(x)
对于任意x属于I，有且仅有一个y属于C，使得y=f(x)成立
其中：C为类别集合， I为项集合， f为分类器
分类算法的任务就是构造分类器f
补充 分类型问题往往采用经验性方法构造映射规则，即一般情况下的分类问题缺少足够的信息来构造100%正确的映射规则。
贝叶斯定理 贝叶斯定理解决了一个问题 已知某条件概率，如何得到两个事件交换后的概率，也就是在已知P(A|B)情况下，如何求得P(B|A)
条件概率 $$P(A|B)=\frac{P(AB)}{P(B)}$$
贝叶斯定理公式 $$P(B|A)=\frac{P(A|B)P(B)}{P(A)}$$
朴素贝叶斯 原理 对于给出的待分类项，求解在此出现的条件下各个类别出现的概率，那个最大，就认为此待分类项属于哪个类别
正式定义  $$设x=\left{a{1}, a{2}, &amp;hellip;, a{m}\right}为一个待分类项，a{i}为x的一个属性$$ $$有类别集合：c=\left{y{1}, y{2}, &amp;hellip;, y_{n}\right}$$ $$计算p(y{1}|x), p(y{2}|x), &amp;hellip;, p(y_{n}|x)$$ $$若p(y{k}|x)=max \left{ p(y{1}|x), p(y{2}) \right}，那么x\in y{k}$$  案例代码 # coding=utf8 &amp;quot;&amp;quot;&amp;quot; @File : bayesain_action.py @Contact : 13132515202@163.com @Modify Time @Author @Version @Description ------------ ------- -------- ----------- 2019/10/11 20:56 LiuHe 1.</description>
    </item>
    
    <item>
      <title>机器学习笔记-决策树</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Sat, 05 Oct 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>优缺点 ​ 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
​ 缺点：可能产生过度匹配问题
​ 适用数据类型：数值型和标称型
创建分支伪代码createBranch() ​ 检测数据集中每个子项是否属于同一分类
​ IF so return 类标签
​ ELSE
​ 寻找划分数据集的最好特征
​ 划分数据集
​ 创建分支节点
​ for 每个划分的子集
​ 调用函数createBranch并增加返回结果到分支节点中
​ return 分支节点
一般流程 收集数据：可以使用任何方法
准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
分析数据：可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期
训练算法：构造书的数据结构
测试算法：使用经验树计算错误率
使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好的理解数据的内在含义
信息增益 信息的定义 如果待分类的事务可能划分在多个分类之中，则符号$$x_{i}$$的信息的定义为
$$l(x{i})=-log{2}p(x_{i})$$
其中$$p(x_{i})$$是选择该分类的概率
熵的定义 熵为信息的期望值，在信息论和概率统计中，熵是表示随机变量不确定性的变量。设X是一个取有限个值的离散随机变量，其概率分布为
$$P(X=x{i}) = p{i}. i=1,2,&amp;hellip;,n$$
则随机变量X的熵定义为：
​ $$H=-\sum{i=1}^{n}p(x{i})log{2}p(x{i})$$
熵越大，随机变量的不确定性就越大
条件熵 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望
$$H(Y|X)=\sum{i=1}^{n}p{i}H(Y|X=x_{i})$$
这里，pi=P(X=xi), i=1,2,&amp;hellip;,n
信息增益 信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度
特征A对训练数据集D的信息增益g(D, A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即
g(D,A)=H(D)-H(D|A)
信息增益大的特征具有更强的分类能力，所以在构建决策树时首先选择信息增益最大的特征作为子节点
决策树工作原理 得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个几点上，我们再次划分数据&amp;hellip;&amp;hellip;
所以使用递归的原则来处理数据集
递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类
决策树的生成 ID3算法 输入：训练数据集D，特征集A，阈值δ</description>
    </item>
    
    <item>
      <title>机器学习笔记-k近邻算法距离度量</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k%E8%BF%91%E9%82%BB%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/</link>
      <pubDate>Tue, 01 Oct 2019 19:23:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k%E8%BF%91%E9%82%BB%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/</guid>
      <description>k近邻算法中的距离度量规则 特征空间中两个实例点的距离是两个实例点相似程度的反映。
设特征空间X是n维实数向量空间$$R^{n},x{i},x{j}\in X,x{i}=(x{i}^{1},x{i}^{2},&amp;hellip;,x{i}^{n})^{T},x{j}=(x{j}^{1},x{j}^{2},&amp;hellip;,x{j}^{n})^{T}$$
则$$x{i},x{j}的L_{p}距离定义为$$
$$L{p}(x{i},x{j})=(\sum{l=1}^{n}|x{i}^{(l)}-x{j}^{l}|^{p})^{\frac{1}{p}}$$
这里p&amp;gt;=1.
欧式距离 当上述p=2时，称为欧氏距离，即
$$L{2}(x{i},x{j})=(\sum{l=1}^{n}|x{i}^{(l)}-x{j}^{l}|^{2})^{\frac{1}{2}}$$
曼哈顿距离 当上述p=1时，称为曼哈顿距离，即
$$L{1}(x{i},x{j})=(\sum{l=1}^{n}|x{i}^{(l)}-x{j}^{l}|)$$
距离最大值 当p趋向无穷时，它是各个坐标距离的最大值，即
$$L{\infty}(x{i},x{j})=max{l}|x{i}^{(l)}-x{j}^{l}|$$</description>
    </item>
    
    <item>
      <title>机器学习笔记—K-近邻算法</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 30 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid>
      <description>原理概述 ​ 存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本中特征最相似数据的分类标签，我们只选择样本数据集中前k个最相似的数据，选择k个最相似数据中出现次数最多的分类，作为新数据的分类
k-近邻算法的流程 收集数据：可以使用任何方法
准备数据：距离计算所需要的数值，最好是结构化的数据格式
分析数据：可以使用任何方法
训练算法：此步骤不适应k-近邻算法
测试算法：计算错误率
使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于那个分类，最后应用对计算出的分类执行后续的处理。
KNN分类算法实现步骤 对未知类别属性的数据集中的每个点依次执行以下操作：
1、计算已知类别数据集中的点与当前点之间的距离；
2、按照距离递增次序排序；
3、选取与当前点距离最小的k个点；
4、确定前k个点所在类别的出现频率；
5、返回前k个点出现频率最高的类别作为当前点的预测分类。
KNN分类算法的优缺点 优点 ​ 精度高、对异常值不敏感、无数据输入假定
缺点 ​ 计算复杂度高、空间复杂度高
使用数据范围 ​ 数值型和标称型
总结 ​ 明天国庆节，祝伟大的祖国生日快乐。</description>
    </item>
    
    <item>
      <title>机器学习笔记-逻辑回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 23 Sep 2019 15:37:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</guid>
      <description>模型原理 分类变量 ​ 又称定性变量、离散型变量，观测的个体智能属于几种互不相容的类别的一种，一般用非数字来表达其类别，这种数据被称为分类变量。与之相对的是连续变量，即定量变量，是由测量、技术或者统计等得到的，这些变量具有数值特征
#### 常见的分类变量
​ 有序变量：没有数量关系，但有次序关系。比如：收入等级、客户级别
​ 名义变量：即无等级关系，也无数量关系。比如：性别、天气等
因变量为分类变量的特点 ​ 误差项非正态分布：误差分布为两点型离散分布，不是正态分布
​ 误差项零均值异方差：均值为0，误差的方差依赖于自变量，会随着自变量的不同水平而变化，是异方差
​ 回归方程限制：y为0-1型随机变量
改进 ​ 回归方程改用限制在[0,1]之间的连续曲线，而不再是直线回归方程，常用的是Logisitic函数
​ 因变量取值为0，1，不适合直接作为回归的因变量，改用因变量取1的概率Π作为回归因变量</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（三）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link>
      <pubDate>Thu, 19 Sep 2019 19:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid>
      <description>机器学习
线性回归分析 回归 ​ 回归是处理两个或两个以上变量之间互相依赖的定量关系的一种统计方法和技术，变量之间的关系并非确定的函数关系，通过一定的概率分布来描述
线性 ​ 线性的严格定义是一种映射关系，其映射关系满足可加性和其次性。通俗理解就是两个变量之间存在一次方函数关系，在平面坐标系中表现为一条直线。不满足线性即为非线性。
线性回归 ​ 线性回归：在回归分析中，如果自变量和因变量之间存在着线性关系，则被称为线性回归。如果只有一个因变量一个自变量，则称为一元线性回归，如果有一个因变量多个自变量，则被称作多元回归。
回归模型 ​ 回归模型的一般形式是：
​ $$y=f(x{1},x{2},x{3},&amp;hellip;.,x{p})+\varepsilon$$
​ f()函数为确定性关系，后者为随机误差
建立回归模型 流程 ​ 1.需求分析明确变量：了解相关需求，明确场景，清楚需要解释的指标（因变量），并根据相关业务知识选取与之有关的变量作为解释变量（自变量）
​ 2.数据收集加工：根据上一步分析得出的解释变量，去收集相关的数据（时序数据、截面数据等），对得到的数据进行清洗、加工，并根据数据情况调整解释变量，并判断是否满足基本假设。
​ 3.确定回归模型： 了解数据集，使用绘图工具绘制变量样本散点图或使用其他分析工具分析变量间的关系，根据结果选择回归模型，如：线性回归模型、指数形式的回归模型等。
​ 4.模型参数估计：模型确定后，基于收集、整理的样本数据，估计模型中的相关参数。最常用的方法是最小二乘法，在不满足基本假设的情况下还会采取岭回归、主成分回归、偏最小二乘法等
​ 最小二乘法：也叫做最小平方法，通过最小化误差的平方和寻找数据的最佳函数匹配的方法
​ 5.模型检验优化：参数确定后，得到模型。此时需要对模型进行统计意义上的检验，包括对回归方程的显著性检验、回归系数的显著性检验、拟合优度检验、异方差检验、多重共线性检验等。还要结合实际场景，判断该模型是否具有意义。
​ 6.模型部署应用
回归模型的特点 ​ 模型简单，建模和应用都比较容易
​ 由坚实的统计理论支撑
​ 定量分析各变量之间的关系
​ 模型预测结果可以通过误差分析精确了解
​ 但：
​ 假设条件比较多且相对严格
​ 变量选择对模型影响较大</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（一）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%80/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%80/</guid>
      <description>机器学习第零章  
机器学习流程  输入数据 特征工程 模型训练 模型部署 模型应用  相关基本概念 输入空间 ​ 将输入的可能取值的集合称作输入控件
输出空间 ​ 将输出的所有可能取值的集合称作输出空间
特征空间 ​ 特征：即属性。每个输入实例的各个组成部分称作原始特征，基于原始特征还可以扩展出更多的衍生特征
​ 特征向量：由多个特征组成的集合，称作特征向量
​ 特征空间：将特征向量存在的空间称作特征空间
假设空间 ​ 假设空间：由输入空间到输出空间的映射的集合，称作假设空间
机器学习方法三要素 方法 = 模型 + 策略 + 算法 模型： ​ 输入空间到输出空间的映射关系
策略： ​ 从假设空间众多的假设中选择到最有的模型的学习标准或规则
算法： ​ 学习模型的具体的计算方法，通常是求解最优化问题
模型分类 ​ 预测分类：分类 Classification
​ 预测取值：回归 Regression
​ 发现结构：聚类 Clustering
​ 发现异常数据：异常检测 Anomaly Detection
损失函数 ​ 用来衡量预测结果和真实结果之间的差距，其值越小，代表预测结果和真实结果越一致
​ 平方损失函数：预测结果与真实结果差的平方
​ $$ L(Y,f(x)) = (Y - f(x))^{2} $$</description>
    </item>
    
    <item>
      <title>机器学习笔记-基本概念（二）</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BA%8C/</link>
      <pubDate>Wed, 18 Sep 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BA%8C/</guid>
      <description>机器学习笔记 
模型选择的原则 误差 ​ 是模型的预测输出值与其真实值之间的差异
训练 ​ 通过已知的样本数据进行学习，从而得到模型的过程
训练误差 ​ 模型作用与训练集时的误差
泛化 ​ 有具体的、个别的扩大为一般的，即从特殊到一半，成为泛化。对机器学习的模型来讲，泛化是指模型作用于新的样本数据（非训练集）
泛化误差 ​ 模型作用于新的样本数据时的误差
模型容量 ​ 是指拟合各种模型的能力
过拟合和欠拟合 ​ 是某个模型在训练集上表现很好，但是在新样本上表现差。模型将训练集的特征学习的太好，导致一些非普遍规律被模型接纳和体现，从而在训练集上表现好，但是对于新样本表现差。反之则成为欠拟合，即模型对训练集的一边性质学习较差，模型作用于训练集时表现不好
模型选择 ​ 针对某个具体的任务，通常会有多种模型可供选择，对同一个模型也会有多组参数，可以通过分析、评估模型的泛化误差，选择泛化误差最小的模型
模型的评估方法 评估思路 ​ 通过实验测试，对模型的泛化误差进行评估，选出泛化误差最小的模型。待测数据集全集未知，使用测试集进项泛化测试，测试误差即为泛化误差的近似
​ 测试集和训练集尽可能互斥
​ 测试集和训练集独立同分布
留出法 ​ 将已知数据集的划分极可能保持数据分布一致性，避免因数据划分过程引入为的偏差
​ 数据分割存在多种形式会导致不同的训练集、测试集划分，单次留出法结果往往存在偶然性，其稳定性较差，通常会进行若干次随机划分、重复实验评估取平均值作为评估结果
​ 数据集拆成两部分，每部分的规模设置会影响评估结果，测试、训练的比例通常为7：3
​ 测试集和训练集分开，缓解了过拟合，但一次划分，评估结果偶然性大，而且数据被拆分后，用户训练、测试的数据更小了
交叉验证法 ​ 将数据集划分k个大小相似的互斥的数据子集，子集数据尽可能保证数据分布的一致性（分层采样），每次从中选取一个数据集作为测试集，其余用作训练集，可以进行k次训练和测试，得到评估均值。该验证方法也称作k折交叉验证。使用不同的划分，重复p次，称作p次k折交叉验证
留一法 ​ 是k折交叉验证的特殊形式，将数据集分成两个，其中一个数据集记录条数为1，作为测试集使用，其余记录作为训练集模型。训练出的模型使用全部诗句集训练得到的模型接近，其评估结果比较准确。缺点是当数据集较大时，训练次数和规模较大。
​ 充分利用了所有样本，多次划分，评估结果相对稳定，但是计算比较繁琐，需要进行k次训练和评估
自助法 ​ 是一种产生样本的抽象方法，其实质是有放回的随机抽样。级从已知数据集中随机抽取一条数据，然后将该记录放入测试集同时放回原数据集，继续下一次冲一样，知道测试集中的数据条数满足要求。
​ 对总体的理论分布没有要求，但是无放回抽样引入了额外的偏差
几种方法的选择 ​ 已知数据集数量充足时，通常采用留出法或者k折交叉验证法
​ 对于已知数据集较小且难以有效划分训练集/测试集的时候，采用自助法
​ 对于已知数据集较小且可以有效划分训练集/测试集的时候，采用留一法
假设检验步骤 1.建立假设 ​ 根据具体的问题，建立假设</description>
    </item>
    
    <item>
      <title>吴恩达机器学习——单变量线性回归</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>吴恩达机器学习——单变量线性回归
基本概念 ### 假设函数  ​ 我们得到一个训练集，并用其进行某种预测时，我们可以根据训练集得出一个假设函数。
​ 如下图所示，下图为某种行业效益值y与程视人口数量x的训练集
​ 根据图像特征，我们可以得出这些数据可以用下图函数来代表：
​ ​ 因此我们可以得出一个假设函数：
​
$$ h{\theta}=\theta{0}+\theta_{1}X $$ ​
代价函数 ​ 在作出假设函数之后，我们要对建设函数中的两个未知量$$\theta{0}$$和$$\theta{1}$$做出选择，即选择不同的$$\theta{0}$$$$\theta{1}$$，验证所选择的值是否能时假设函数最大程度的能跟数值相符合
​ 对于如何判断所选择的$$\theta{0}$$和$$\theta{1}$$时最好的，我们选择均方误差来作为衡量标准，即： $$ J(\theta)=\tfrac{1}{2m}\sum{i=0}^{m}(h{\theta}(x^{i})-y^{i})^{2} $$ ​ 此即为我们的代价函数，函数值绝对值越小表示$$\theta{0}$$和$$\theta{1}$$越符合目标值
梯度下降算法 前言 ​ 在求代价函数的最小值时，以二维图像来为例表示函数值变化规律来讲，假设函数值变化规律如下图所示
​ 1处为最小值点，我们发现如果根据函数特征看，当$$\Delta x$$与$$y&amp;rsquo;$$符号相反时，沿着这个方向可以从任何点到达最小值处，因此在学习过程中可以根据导数（当为多元函数时为梯度）来判断如何改变变量值，来是的求解过程更加快速。
梯度定义 ​ 梯度表示某一函数在该处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向变化最快，变化率最大
多元函数梯度表达式 $$ \bigtriangledown f = (\frac{\partial f}{\partial x{i}}) 其中(\bigtriangledown f){i}=\frac{\partial f}{\partial x_{i}} $$
根据梯度下降法则选择$$ \theta$$ ​ 在选取变量$$\theta$$值时，根据梯度下降法则应该按照以下规则选取$$\theta$$的值
​
$$ \theta = \theta - \alpha \bigtriangleup f $$ ​ 其中$$\alpha$$为学习率，自己选取,因为梯度的方向是函数f增长最快的方向，梯度的反方向是f降低最快的方向，所以选取梯度的负数方向。
梯度下降算法求解过程 ​ 1.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习——监督学习和无监督学习</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</guid>
      <description>吴恩达机器学习——监督学习和无监督学习 监督学习 特征：  有标签数据 2.直接反馈 3.预测结果/未来  对每个数据都有准确的反馈
两个子类 1.回归：Regression 预测结果为连续值即为回归
2.分类：Classification 预测结果为离散值即为分类
无监督学习 特征： 1.无标签/目标 2.无反馈 3.寻找数据中移仓的结构
两个子类 1.聚类： 见数据按相似度聚类成不同分组
2.降维： 在保留数据结构和有用性的同时对数据进行压缩</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(matlab)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</guid>
      <description>吴恩达机器学习——单变量线性回归作业（matlab） 1.Simple Octave/MATLAB function warmUpExercise.m：生成一个5 * 5 的单位矩阵 实现代码：
A = eye(5)	 笔记:matlab中eye(n)函数生成一个n*n的单位矩阵
结果：
2.Plotting the Data plotData.m：绘制数据，将数据集绘制在二维空间 实现代码:
plot(x,y,&#39;rx&#39;,&#39;MarkerSize&#39;,10); %Plot the data ylabel(&#39;Profit in $10,000s&#39;);	%set the y-axis label xlabel(&#39;Pupulation of City in 10,000s&#39;); %set the x-axis label  笔记：plot(x,y,&amp;lsquo;rx&amp;rsquo;,&amp;lsquo;MarkerSize&amp;rsquo;,10)参数含义:x、y为点的坐标，rx表示用红色的X形状来表示点，MarkerSize、10表示X形状的大小
结果：
​ ​
3.Computing the cost J(theta) computeCost.m：计算代价函数值 假设函数： $$ h{\theta}(x)=\theta^{\tau}x=\theta{0}+\theta{1}x $$ 代价函数： $$ J(\theta)=\tfrac{1}{2m}\sum{i=1}^{m}(h_{\theta}(x^{i})-y^{i})^{2} $$ 实现代码：
J = sum(((X * theta) - y) .^ 2)/(2*m);  笔记：.</description>
    </item>
    
    <item>
      <title>吴恩达机器学习—单变量线性回归作业(python)</title>
      <link>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</link>
      <pubDate>Fri, 16 Aug 2019 14:43:58 +0800</pubDate>
      
      <guid>https://okery.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9Apython/</guid>
      <description>机器学习—单变量线性回归作业(python) python代码实现链接： 代码</description>
    </item>
    
  </channel>
</rss>